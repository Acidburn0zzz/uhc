%%[main

\chapter{Recursion}
\label{chapt.Recursion}

In Chapter~\ref{fill this in - mono let}, we discussed inferencing of uniqueness types for a non-recursive
Let. We go one step futher in this chapter, in order to support a (mutual) recursive Let. It is important
to fully understand Chapter~\ref{fill this in - mono let} before proceeding with this chapter, especially
the part about the inferencer (Section~\ref{fill this in} and Section~\ref{fill this in}), since we set up
the type system by means of the |Inst| constraint in such a way that only the inferencer is affected to
support recursion.


\section{Example}

  Consider the following, incorrect\footnote{We discuss how to make expressions conditional in Chapter~\ref{fill this in} about data types},
  implementation of the faculty function:

%%[[wrap=code
let  (Sub(fac)(a)) :: Int -> Int = \n -> n * (Sub(fac)(b)) (n - 1)
in   fac 3
%%]

  So, what is the challenge of the recursion? The problem is that when gathering the constraint set of (the binding group of) |(Sub(fac)(a))|,
  we encounter the variable |(Sub(fac)(b))|, which results into an |Inst| constraint that refers to |(Sub(fac)(a))|. So, the constraint
  graph of |(Sub(fac)(a)| is needed to construct the constraint graph of |(Sub(fac)(a)|, which is a problem, since it is not yet there.

  This is a familiar problem. This problem already occurs with a conventional type system. Hindley-Milner type inference solves this
  problem by treating an identifier, that occurs recursively, monomorphically. That is also a solution solution to our problem. By
  giving up a polyvariant typing for an identifier occuring in an expression of it's own binding group, we do not need the constraint
  graph when dealing with the corresponding |Inst| constraint (Section~\ref{fill this in}). A downside is that we loose some
  accurracy this way~\cite{fill this in - accuracy loss withour fixpoitn iteration}. In a conventional type system, this problem can
  be solved when a programmer provides a polymorphic type signature~\cite{fill this in - higher ranked typing}. Similary, we can bypass
  this problem, when the programmer supplies a polyvariant constraint set (Section~\ref{fill this in}).

  But, countrary to the conventional typing problem, full polyvariant uniqueness type inference is possible in the presence of
  recursion, by means of a pixpoint iteration on the constraint graph, which assumes initially an empty constraint graph for
  the binding group and keeps expanding the graph, until a fixpoint is reached. We come to this in Section~\ref{fill this in}.


\section{Monovariant recursion}
\label{Sect.MonovariantRecursion}

  A solution is to forget instantiation for recursive identifiers, and choose as use-site annotated type, the definition-site
  annotated type. For example, for the |fac| function:

%%[[wrap=code
let  fac :: (Annot(Int)(2)) (Annot(->)(1)) (Annot(Int)(3)) = \n -> n * (fac :: (Annot(Int)(2)) (Annot(->)(1)) (Annot(Int)(3))) (n - 1)
 in  fac :: (Annot(Int)(5)) (Annot(->)(4)) (Annot(Int)(6))
%%]

  In the type system, this can be accomplished by equating each pair in an |Inst| constraint that refers to the same binding
  group as it is defined in. An |Inst| constraint contains a pairs |(a, b)|, with |a| as the definition-site uniqueness annotation,
  and |b| the use-site uniqueness annotation. By generating the constraint |a =>= b| and |b =>= a| for each pair |(a, b)|, the two
  uniqueness variables are effectively forced to have the same values.

  There is a slight complication in the presence of nested binding groups. An |Inst| constraint is in binding group |n| when it
  generated from any expression in the right-hand side of a binding of binding group |n|. In the presence of nested binding groups,
  an |Inst| constraint can be in multiple binding groups. The binding group in which it is created, and all the parents of that
  binding group.

  To support recursion, only a minor change of the inferencer is required. But, in this approach, individual recursive identifiers
  are treated in exactly the same way. This gives suboptimal typings for some, perhaps contrived, programs~\cite{fill this in}, such as:

%%[[wrap=code
let  f = \n ->  let  x = x
                in   f x + f (n-1)
in   f 1
%%]

  What happens here is that the |x| is discovered to be shared. It is passed as argument to |f|, which makes the parameter of |f|
  shared. The |n| is also a argument of |f|, and since the parameter of |f| is considered to be shared, it makes |n| shared as
  well, altough |n| can be unique as well.


\section{Helping the inferencer with manual constraint sets}

  In Section~\ref{fill this in}, we bypassed the problem of needing a constraint graph while building it, by removing the
  requirement of the constraint graph. This approach has limitations, although it is questionable if these limitations
  are ever a problem in practice. However, we can consider what the impact on the type system is in circumventing this limitation.

  Instead of dropping the requirement on constraint graph |A| while building |A|, we can change the requirement on |A| to some
  other graph |B|, which is at least as restrictive as |A|. This means that any valid substitution of |B| is also
  a valid substitution of |A|, and can replace |A|. When constructing the graph of |A|, the type system can just insert a copy
  of |B| where |A| is expected. This basically shifts the problem to another place, since we still need this graph |B|, but
  we come to that in a second.

  How can we tell the type system to use another graph for a certain identifier? The |Inst| constraint helps us again. Suppose
  that for some identifier |f :: utau|, we have a constraint set |S|, where |S| does not contain any |Inst| constraints. We then
  create some new binding group |n| for |S|, and for each use |f :: (Sub(utau)(2))|, generate an |Inst| constraint that refers to
  |n| and maps uniqueness variables of |utau| to |(Sub(utau)(2))|. Then the constraint graph obtained from |S| is used as
  replacement for the constraint graph of the binding group of |f|.

\subsection{Programmer specified constraint sets}

  The constraint set is specified by the programmer. The programmer gives annotated types of the recursive identifiers in the binding group,
  and a constraint set. For example:

%%[[wrap=code
  f :: a:Int -> 1:Int, 1 =>= a
%%]

  Each type constructor is prefixed with a uniqueness variable chosen by the programmer, a concrete uniquness value, or nothing. A
  concrete uniqueness value |v| is converted into a fresly invented variable |delta|, together with a special equality
  constraint |v === delta|. We can represent the |===| constraint internally with two |=>=| constraints without coercions. Specifying
  no annotation has the same meaning as the concrete uniqueness annotation |*|.

  To construct the desired constraint set |S|, the type given by the programmer is extended with uniqueness variables for the concrete or
  missing annotations, and the constraint set of the user updated accordingly. By overlaying the type of the programmer on the type
  inferred for the identifier, the uniquness variables in the constraints are renamed to the actual uniqueness variables, instead of
  the symbols of the programmer. The result is |S|.

\subsection{The problem with mono uniqueness variables}

  There is a problem with |S|. Binding groups are not independent. Mono uniqueness variables occur all over the place, and
  put links between binding groups. Unfortunately, the programmer cannot specify the effects on the mono-variables, since they do not have
  a name where the programmer can refer to. Conventional type signatures have this problem as well, as the following example shows:

%%[[wrap=code
let f :: forall alpha . alpha -> alpha
    f x =  let  y :: ???
                y = x
           in y
%%]

  In Haskell, we are unable to specify the type for |y| in this case. A solution is to give a name to the skolemnized
  version of type variable |alpha|, such that this name can be used in the signature of |y|. Unfortunately, the dependencies
  in our case a much higher, requiring naming all the mono variables that are affected by the expression for which we want to
  specify the constraint set.

  On the other hand, if we are talking about a signature for a top-level function, then virtually all mono
  variables will be annotations on the function spine of toplevel functions. By enforcing with constraints on the toplevel
  bindings that the function spine of a toplevel function is shared, we can eliminate the need to specify the dependencies
  on mono-variables. This is not a very bad assumption to make, since from a design perspective, a function should only be
  toplevel if it is intended to be shared by multiple functions or modules. With this approach, programmer specified constraint
  sets are feasible.

\subsection{Entailment check}

  Unfortunately, there is another problem: Suppose we have such an |S|. How do we know that the programmer specified the right constraint
  set? For example, how can we be sure that the constraint set of the programmer does not forget a constraint between some uniqueness
  variables? For that, the compiler has to prove that |S| \emph{entails} |S'|, where |S'| is the constraint set resulting from
  constraint gathering on the expression. This means that a substituation on the sacred uniqueness variables, that is valid for
  |S|, should also be valid for |S'|.

  As a side note, the word \emph{substitution} is a bit ambiguous here, since there are two solve phases. Normally, we talk about the final
  substitution, so the substitution resulting from the uniqueness propagation phase. In this case, however, it is enough that
  we can prove that |S| entails |S'|, for substitutions resulting from the reference-counting phase, since if |S'| is more
  restrictive in the reference count that |S|, it also is more restrictive in the uniqueness values.

  One way to prove that the constraint set is valid, is to enumerate each combination of uniqueness values and check the implication |S => S'|, but this is
  not practical in practice. For propositional logic, there are many satisfiability checkers that can help us out here, but our
  final constraint set (which only have |=>=| and |\+/| constraints) is an expression in a three-valued logic. Fortunately, we
  can translate the constraints to propositional logic.

  For each uniqueness variable |delta|, we require two propositional symbols, |P| and |Q|:

  \begin{tabular}{l||ll}
  |delta| & |P| & |Q| \\
  \hline
  0 & 0 & 0 \\
  1 & 0 & 1 \\
  * & 1 & 0 \\
  - & 1 & 1 \\
  \end{tabular}

  With this scheme, we can construct truth-tables for the constraints, and use a well-known
  trick to obtain a propositional expression~\cite{how the hell should I know}:

  \begin{tabular}{ll||l||llll||l}
  |p| & |q| & |p =>= q| & |(Sub(p)(1))| & |(Sub(p)(2))| & |(Sub(q)(1))| & |(Sub(q)(2))| \\
  \hline
  0 & 0 & 1 & 0 & 0 & 0 & 0 & $\neg{p_1} \wedge \neg{p_2} \wedge \neg{q_1} \wedge \neg{q_2}$ \\
  0 & 1 & 1 & 0 & 0 & 0 & 1 & $\neg{p_1} \wedge \neg{p_2} \wedge \neg{q_1} \wedge q_2$ \\
  0 & * & 1 & 0 & 0 & 1 & 0 & $\neg{p_1} \wedge \neg{p_2} \wedge q_1 \wedge \neg{q_2}$ \\
  1 & 0 & 0 & 0 & 1 & 0 & 0 & $-$   \\
  1 & 1 & 1 & 0 & 1 & 0 & 1 & $\neg{p_1} \wedge p_2 \wedge \neg{q_1} \wedge q_2$ \\
  1 & * & 1 & 0 & 1 & 1 & 0 & $\neg{p_1} \wedge p_2 \wedge q_1 \wedge \neg{q_2}$ \\
  * & 0 & 0 & 1 & 0 & 0 & 0 & $-$   \\
  * & 1 & 1 & 1 & 0 & 0 & 1 & $p_1 \wedge \neg{p_2} \wedge \neg{q_1} \wedge q_2$ \\
  * & * & 1 & 1 & 0 & 1 & 0 & $p_1 \wedge \neg{p_2} \wedge q_1 \wedge \neg{q_2}$ \\
  \end{tabular}

  The propositional variant of the constraint is the disjunction of the above
  propositions.

  Instead of translating an arbitrary |\+/| constraint directly, we split it first up
  into multiple, but smaller, |\+/| constraints. Since the |\+/| is commutative, and
  the |<=| transitive, we can introduce some fresh uniqueness variables, and shape
  the |\+/| constraints into a tree form:

%%[[wrap=code
... picture ...
%%]

  Each |\+/| constraint has at most three uniqueness variables this way. The translation
  scheme is then similar to the scheme for |=>=|, only three times as big.

  After converting the entire constraint set into propositions, tools from propositional
  logic are used to verify that the proposition holds, or to come up with a counter example.
  Converting such a counter example into a piece of information understandable by the
  programmer is not trivial, and left as a subject for further study.

\subsection{Final remarks about helping the inferencer}

  Helping the inferencer by specifying a replacement constraint set, gives some mixed feelings.
  Technically it is possible, but the effort it takes to get it implemented, is excessively
  huge, compared to the gains.


\section{Full polyvariant recursion}

  There is a way to support full polyvariant recursion. The |Inst| constraints are a specification: is there a finite graph
  |G|, such that when a graph |G'| is constructed using |G| at the places of the corresponding |Inst| constraints, that
  |G'| is equivalent to |G|? The set of sacred annotations is finite, so according to the graph reduction section
  (Section~\ref{fill this in}, we know that any graph for these annotations can be reduced to a finite graph. A graph that always
  fits the specification, is the graph that has an edge between each pair of sacred annotations, and a hyper edge between each
  subset of sacred annotations, including with and without coercions. Of course, a smaller graph is desired.

  To obtain a smallest graph that satisfies the specification, we perform a fixpoint computation on the constraint graph. Each
  binding group initially has an empty graph, and each iteration, the insertion of graphs at the |Inst| constraints, is
  replayed. The graphs are normalized at the end of the iteration. This process proceeds until the graphs do not change anymore.

\subsection{Fixpoint iterate the substitution}

  We can complicate the procedure lightly, such that it converges faster, by not fixpoint iterating on the constraint graph,
  but by fixpoint iterating on the substitution on the sacred uniqueness annotations of the constraint graph. If after a new
  insertion of the graph, the substitution on the sacred uniqueness variables does not change, then there is not a new path
  in the graph (or a subsumed path), and thus a new interation will have no futher effect. An advantage of this approach is
  that the reduction procedure can benefit from reference-count information, and that subsumed paths do not inflict new
  iterations, thus causing faster convergence. In fact, graphs do not have to be reduced to normal form either, so the
  implementation can decide to perform only the quick reduction strategies, and reduction to normal form only when graphs reach
  a certain size. For example, twice as big as the last time we reduced the graph to normal form.

\subsection{Consequences}

  Implementation of this procedure in the inferencer, has only a slight impact. It does not influence the other operations of
  the inferencer much, and given functions that reduce a graph, and functions for dealing with the |Inst| constraint, implementing
  functionality as discussed in Chapter~\ref{fill this in}, this procedure is not very difficult to implement.

  But there is a downside: even with the reduction strategies, the graphs for even toy programs programs are big. Fixpoint
  iterations on these graphs require at least the procedure to be performed twice, and depending on the nesting level and
  amount of recursion, several more times. For most programs, this fixpoint iteration process will be overkill. The question
  is whether this approach will be worth it.

  We would like to give this choice to the programmer. This choice is possible due to the |Inst| constraint. The programmer
  specifies for which binding groups are fixpoint strategy is required. The inferencer uses the implementation of this section
  as main inference strategy. All binding groups are put in the worklist in the earlier descripted topologically sorted order
  (Section~\ref{fill this in}). When a recursive |Inst| constraint is encountered while processing a binding-group, then depending on
  the choice of the programmer, either the quick equation approach of Section~\ref{fill this in} is taken, or the iterative approach
  of this chapter, by reinserting this binding group and the binding groups dependent on it, back in the work list. We can thus have
  best of both worlds, without a performance penalty if the fixpoint approach is never selected.
  
  However, if we support the fixpoint approach, allowing full user-defined constraint sets (Section~\ref{fill this in}), becomes
  a problem. We can no longer verify the constraint set given by the programmer beforehand, since the full constraint set is
  only known at the use-site of an identifier. We can check that the constraint set is correct for each use of an identifier,
  just by checking if the substitution for the use-site is also a valid substitution for the constraints obtained by augmenting
  the programmer-supplied constraint set with the gathered constraint set, but that has many subtilities, due to dependencies
  on other binding-groups, differences between use-site type and definition-site type, and perhaps most important, the problem
  is detected at a very late time, with probably no reasonable way to relate the error back to a location in the constraint
  set specified by the programmer. Letting the programmer specify additional constraints, is not a problem, however (\ref{fill this in - conclusion of help}).


\section{Conclusion}

  The |Inst| constraint plays an important role in dealing with recursion. At least two different ways of dealing with this
  constraint are possibly. One way is fast, but less accurate. One way is accurate, but slow. The constraint creates an
  abstraction barrier between the two ways, allowing both to exist at the same time. In practice, the fast approach
  is virtually always sufficient, but in case the programmer really needs the additional power, it is possible to turn on
  the slow approach for specific parts of the program.

  This chapter also showed the need for constraint graphs and the corresponding reductions of Chapter~\ref{fill this in}. Without the
  reductions, the fixpoint iteration process is not possible for all but the tiniest programs. That said, this almost concludes
  the story about constraint graphs. Only Chapter~\ref{fill this in - polymorphism} about polymorphism will mention constraint graphs again,
  since instantiation to a type with more structure requires another form of duplication of graphs.

%%]

