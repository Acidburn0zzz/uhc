%%[main

\chapter{Adding Data Types}
\label{chapt.DataTypes}

The language that we evolved during the previous chapter still misses many features that a industry-ready
functional programming language has. But from a typing point of view, the feature set is fairly complete,
with only one major feature missing: support for data types. In this chapter, we discuss how to support
data types.

Support for data types involves changes at two levels. The expression level needs support for pattern matches
and case-expressions. At the type level, the types become more complicated, with more annotations, dictated
by the types in a data-type definition. Although data types influence a lot of parts within a computer, the
required changes are surprisingly isolated. The constraint gathering phase uses a slightly different |\+/|
constraint, and has now cases for case expressions and pattern matches. The solver needs to deal with
this altered version of the |\+/| constraint, but that is not much different than we discussed in
Chapter~\ref{fill this in}, so we leave that out. The only place that is heavily affected, is the conversion
to annotation trees (Section~\ref{fill this in}). In order to perform this conversion, it turns out that we
have to perform two analysis on the data type definitions (Chapter~\ref{chapt.Polarity}).

This chapter is organised as follows. We start with an example that illustrate what the problems are to
support the data types. Each of these problems are discussed in a subsequent section in isolation. The
contents of these sections require a fair understanding of the previous chapters. This chapter scales up
the concepts to support the data types, but does not introduce new concepts.


\section{Example}

The example data type that we take in this chapter, are lists:

%%[[wrap=code
data List a  =  Cons a (List a)
             |  Nil
%%]

The data-type definition dictates how the representation of a |List| in memory is. In this case, a
list is a region of memory representing a |Cons| or a |Nil|. In the first case, the area of memory
has two pointers, one to a value of type |a|, and a pointer to the remainder of the list. Each area
of memory has a corresponding type in the data-type definition. The other way around, each type in
a data-type definition, corresponds to zero or more memory regions of a value of that data type.
Similar to Section~\ref{fill this in}, annotations on the type constructors of the data type, classify
the uniqueness properties of a piece of memory. This example also shows why annotations on the types
are useful: a value can be infinite, but the representation of the type is finite.

With the annotation problem taken care of, the existing infrastructure knows how to deal with
expressions with user-defined data types as types. But, in order to construct and deconstruct such
values we need a few alterations in the expression language, by means of value constructors,
pattern matching, and case-expressions. Value construtors are just functions with a type derived
from the corresponding data-type definition. We already know how to deal with those. But
pattern matches and case-expressions deserve some attention.

The annotations are important in the case of pattern matching. Pattern matching touches the spine of
a value, and if it succeedes, can bring identifiers into scope. Constraints need to be generated
in order to properly flow use-counts into the annotations on the spine of data types. Annotations
for the types of the identifiers introduced by the pattern match, needs to be obtained from
the type of the pattern and the corresponding data-type definition (Section~\ref{fill this in}).
For example, if some value has type |(Annot(tau)(0)) === (Annot(List)(1)) (Annot(Int)(2))|, and we perform a successful
pattern match on |Cons (x :: (Annot((Sub(tau)(1)))(3)) (xs :: (Annot((Sub(tau)(2)))(4)))|, then each use
of |x| and |xs| can potentially result in a use of the spine of the value towards the location where
|x| or |xs| is stored, so we generate the constraints: |(Delta(3)) (Sub(=>=)(s)) (Delta(0))| and |(Delta(4)) (Sub(=>=)(s)) (Delta(0))|.
For the strict parts in the pattern, in this case the |Cons| constructor alone (with |(Delta(0))| being the corresponding
annotation), we generate a constraint |delta =>= (Delta(0))|, where |delta| is then number of uses of the expression containing the
pattern match.

To support case expressions, we need to support patterns first. But then there is still something special about
cse constructions with regards to the expressions occurring in the branches. Only the expression connected to
the first pattern match that succeeds, is evaluated. Consider the following example:

%%[[wrap=code
\b (x :: utau) -> case b of
                    True   -> (x :: (Sub(utau)(1))) + (x :: (Sub(utau)(2)))
                    False  -> (x :: (Sub(utau)(3)))
%%]

The constraint gathering of Section~\ref{fill this in}, generates the constraint |(Sub(utau)(1))) \+/ (Sub(utau)(2))) \+/ (Sub(utau)(3))) <= utau|.
This constraint is overly conservative. We only need to add up the occurrences within a single execution path, and take the maximum over
all execution paths. Suppose that |\-/| stands for maximum, then the constraint is in this case: |((Sub(utau)(1))) \+/ (Sub(utau)(2)))) \-/ (Sub(utau)(3))) <= utau|.
The result will be that we put a hyper-edge in the constraint graph that contains a addition/max computation, instead of only additions.

We consider these changes in isolation.


\section{Exposed annotations}

  Data types complicate the annotation process. Consider the following data-type definition:

%%[[wrap=code
  data Tupple = T Int Int
%%]

  Only the |Tupple| type constructor gets an annotation |delta|. But what annotations to give to both integers? There are several ways to deal
  with this situation. A straightforward was is to annotate each |Int| with |delta|. That way we treat the components of a data type the same
  as the data type itself. Although allowed, this does not provide the accurracy we aim for. A second way is to ask the programmer for the
  annotations on the components of the type, and assume that they are shared if that is not the case. This is the approach taken by
  Clean~\cite{fill this in}. The third way is to expose the annotations on inner types for choosing by the outside world~\cite{fill this in - spj paper}:

%%[[wrap=code
  data (Dot(Tupple)(delta)([(Sub(delta)(1)), (Sub(delta)(2))])()) = T (Dot(Int)((Delta(1)))({})()) (Dot(Int)((Delta(2)))({})())
%%]

  This way, the use-site can choose if the |Tupple| contains two unique integers, a unique and
  a shared integer, or two shared integers. The list subscripted on a type constructor we call
  the set of exposed annotations. This list contains all annotations occurring inside the
  data-type definition.

  This approach is slightly complicated in case of (mutual) recursive data types, since what list of
  annotations should we pass to the recursive type constructor? This is a familiar problem, for which there is
  a common solution: in a first pass, pick fresh annotations for type constructors from another
  binding group, and take the empty list for annotations from the same binding group. Collect
  all the exposed annotations for each binding-group. In the second pass, set the exposed annotations
  for type constructors of the same binding-group to the list of the entire binding-group. Now, each
  annotation occurring inside the data-type definition, occurs in the list of exposed annotations of
  the corresponding type constructor.

  For example:

%%[[wrap=code
  data (Dot(InfIntList)(delta)([(Delta(1)),(Delta(2))])()) = Cell (Annot(Int)(1)) (Dot(InfIntList)((Delta(2)))([(Delta(1)),(Delta(2))])())
%%]

  But what about named type variables in a data-type definition? The problem with type variables is that the
  structure of a type variable can be substituted to virtually any structure at the use-site of a
  data type, and we cannot give the annotations beforehand. But that is not required either: the annotated
  types passed as arguments to a type constructor, contain exactly the annotations that we did not know yet.
  For example:

%%[[wrap=code
  data (Dot(Embed)(delta)([])()) a = Emb a
%%]

  The annotations to choose for |a| come from the use-site. For example, the type |(Dot(Embed)((Delta(1)))([])()) (Annot(Int)(2))|
  specifies that a value of this type has |(Delta(1))| as annotation on Emb constructor, and |(Delta(2))| on the integer
  that it contains.

  \subsection{Implementation}

  After obtaining the set of annotations for each data type, the implementation of this approach appears to be straightforward
  after the work that we did for annotation trees in Section~\ref{fill this in}. When annotating a type constructor in a
  type, pick fresh annotations for the exposed annotations, and when processing a constraints with these type in them,
  construct an annotation tree with the list of annotations as attachment on the corresponding node in the annotation
  tree. The treatment of exposed annotations in the annotation tree is the same as the other annotations in the
  annotation trees. There is a complication, however, since we need to know the variance of annotations and the fact
  if they are below a function arrow or not. These two subjects are more involved in the presence of data types.
  Chapter~\ref{fill this in} provides a way to obtain the required information about the location of annotations.

  \subsection{Conclusion}

  Thus, this approach is flexible and the changes are fairly isolated. There is a downside to this approach, which is
  that types take a lot of annotations this way. We can control this number of annotations by giving up accurracy, for
  example by enforcing that the annotations of a certain data type are never taken freshly. This saves annotations when
  the data type is used a lot, but has as consequence that there is only one uniqueness variant of it, such that a value
  |v| of such a type has a component that is used in a unique fashion, and there is a value |w| of this type of which that
  component is shared, then the component of |v| is also considered shared, although it could be unique.

\section{Expanded types}

  There are limitations on the exposed annotations approach. If we look at a list, then each |Cons| in the list is
  treated the same way. We, for example, cannot make a distinction between certain elements of a list. On the other
  hand, the reason for taking a list (which is of varying size), is to treat each element in the same way, so that
  is not really an issue. A bigger limitation is that a type variable can only be used in one fashion. For example:

%%[[wrap=code
  data Tupple a = T a a
%%]

  In this example, both occurrences of |a| will get the same uniqueness properties, since we take the annotations from
  the type passed to |Tupple|. But in memory, each |a| is a separate area and thus can have different uniqueness
  properties. So, can we change the system such that we can treat occurrences of the same type variable differently?

  We can achieve this by using an ad-hoc trick: by unfolding the data-type definitions a couple of times. For
  example, unfolding |Tupple Int| results into the type |<T Int Int>|, and can be annotated with annotations in the
  conventional way.  The value-constructor name does not need an annotation, but is part of the type in order to
  determine of which constructor the fields are an expansion. Type expansion is dangerous for recursive types, since
  the expansion process will not terminate, and the representation can explode quickly, especially for nested
  data types.

  The expansion approach and the exposed-annotations approach can be combined. The exposed-annotations approach can
  be used on any type, and the expansion approach on any type of kind |star|. By default, due to the dangerous nature
  of expansions, we do not perform expansions and require the programmer to specify when an expansion is allowed. Assume
  that |utau| is a type annotated by the exposed-annotations approach. Consider that during a topdown visit a type
  |(Sub(utau)(0))| is encountered, with outermost type constructor |T| and parameters |(Sub(utau)(1))|, ..., |(Sub(utau)(n))|.
  If |(Sub(utau)(0))| is of kind |star| and the occurrence of |T| in |utau| is allowed for expansion, then the definition
  of |T| is unfolded with a freshly annotated version of |(Sub(utau)(i))| at each occurrence of type variable |i|. This
  procedure thus works by performing one-step expansions.

  For example, consider the data-type definition of |List|:

%%[[wrap=code
  data (Dot(List)((Delta(1)))((Delta(2)))()) a  = Cons a ((Dot(List)((Delta(2)))((Delta(2)))()) a)
                                                | Nil
%%]

  Suppose that we annotate the type |List Int|. By means of exposed annotations, we obtain the
  type:

%%[[wrap=code
  (Dot(List)((Delta(1)))((Delta(2)))()) (Dot(Int)((Delta(3)))([])())
%%]

  A one step expansion gives:

%%[[wrap=code
<Cons (Dot(Int)((Delta(4)))([])()) ((Dot(List)((Delta(2)))((Delta(6)))()) (Dot(Int)((Delta(5)))([])())), Nil>
%%]

  Note how the exposed annotation |(Delta(2))| is used on his corresponding occurrence on the
  data type during the expansion. Expanding yet another step further, gives:

%%[[wrap=code
<Cons (Dot(Int)((Delta(4)))([])()) <Cons (Dot(Int)((Delta(7)))([])()) ((Dot(List)((Delta(6)))((Delta(9)))()) (Dot(Int)((Delta(8)))([])())), Nil>, Nil>
%%]

  These one-step expansions are not only used for annotating the data types, but also for expanding a type for a
  certain constructor in pattern matches, and in constructing an equivalent annotation tree structure if one
  type has exposed type, and the other an expanded type. So, in a way, the infrastructure that is needed to support
  expanded types, is already needed for the system itself.

  So, how does a programmer specify when a type may be expanded? We use \emph{path expressions}, which is a
  sequence of constructor names with wildcards. A type constructor |T| is allowed to be expanded, when |T|
  occurs in some expanded type, which as parents |(Sub(T)(1)), ..., (Sub(T)(n))|, and there is a prefix of
  a constructor sequence equal to |(Sub(T)(1)), ..., (Sub(T)(n)), T|. It must explicitly mention |T|, but
  it is allowed to have a wildcard for some of the constructor names.

  A path expression is defined by:

%%[[wrap=code
  path  ::=  T path   -- constructor
        |    *T path  -- wildcard
        |    empty
%%]

  A wildcard stands for any number of constructors not equal to the constructor after the wildcard. For
  example, the sequence |* List * List * List| specifies that a list may be unfolded thrice.

  With expanded types, two things can be accomplished: type constructors of kind |*| kan get different
  annotations for each occurrence in the body of a data type, and the head of a data type can be
  annotated differently than the (possibly infinite) tail. This power comes at a price: the types grow
  and so does the number of annotations. Therefore, expanded types are off by default, only to be used
  in special circumstances, specified by the programmer by path expressions.

\section{Pattern matches}

  Pattern matches bring identifiers into scope, and force evaluation of a certain part of a value to
  determine if a pattern matches or not.

  The structure of a pattern match dictates how to pattern match on a type to obtain the type of
  the subpatterns. An expanded type (Section~\ref{fill this in}) can directly be overlayed on top
  of a pattern match. An exposed-annotated type can be overlayed on a pattern match after a
  one-step expansion for each pattern match on a constructor. For example, if we take the one-step
  expansion of Section~ref{fill this in} as example, then a match |Cons x xs|, with the type of
  the entire pattern match being |<Cons (Dot(Int)((Delta(4)))([])()) ((Dot(List)((Delta(2)))((Delta(6)))()) (Dot(Int)((Delta(5)))([])())), Nil>|,
  gives |(Dot(Int)((Delta(4)))([])())| for |x| and |(Dot(List)((Delta(2)))((Delta(6)))()) (Dot(Int)((Delta(5)))([])())| for
  |xs|. So, for pattern matches, we the annotated type is pushed inwards, expanding it if needed for
  each match on a constructor.

  A pattern match cannot obtain a component from a value without evaluating the spine of that
  value. The reference-counts of the spine are thus at least the reference-count of any
  component. Define the parent of a pattern to be the match on a constructor closest to the
  pattern. For example, in the match |(Sub(Cons)(1)) x ((Sub(Cons)(2)) y ys)|, the parent of
  both |y| and |ys| is |(Sub(Cons)(2))|, and the pattern of |(Sub(Cons)(2))| and |x| is
  |(Sub(Cons)(1))|. Associated with each variable binding, and each constructor, is the
  type of the value that on which the pattern is applied. Suppose |(Delta(1))| is the
  outermost annotation on a constructor match or variable binding, and |(Delta(2))| is the
  outermost annotation on the type of the value of the corresponding parent. We generate a
  constraint |(Delta(1)) (Sub(=>=)(s)) (Delta(2))| to propagate reference counts. Due to
  transitivity in the |=>=| constraint, each pattern is connected to the parent, the
  parent of the parent, and so on.

  This approach can be refined a bit, depending on guarantees of the code-generation process.
  If the code generator guarantees that a pattern match is only evaluated once for each binding
  (which is a valid assumption with call-by-name semantics), then for each parent pattern, we can take the
  \emph{minimum} between the reference-count via the conventional way, and the reference-count of the
  expression tied to the pattern match. In other words, suppose that |delta| is the annotation on
  a pattern, |(Sub(delta)(par))| the annotation on the corresponding parent, and |(Sub(delta)(expr))|
  the annotation on the expression tied to the pattern match. Then generate the constraint:
  |delta `min` (Sub(delta)(expr)) <= |Sub(delta)(par))|.

  The other aspect of pattern matches, strictness, is resolved by propagating reference counts from
  the expression tied to the pattern match to the matches on constructors. If |delta| is the outermost
  annotation of the type of the value on which a \emph{refutable| constructor match is applied, and
  |(Sub(delta)(expr))| is the outermost annotation on the type of the corresponding expression, then
  we generate a constraint |(Sub(delta)(expr)) (Sub(=>=)(s)) delta| to capture the strictness effects
  of a pattern match.

\section{Case expressions}

  Constraint gathering for case expressions is fairly straightforward now we know how to deal with
  pattern matches. For the pattern matches and the subexpressions, we apply the constraint
  gathering rules as discussed before. A freshly annotated type |utau| is invented for the result
  type |tau| of the case-expression. Each branch |i|, with annotated type |(Sub(utau)(i))|, is coerced
  to the result type of the case-expressions by generating the constraint |utau =>= (Sub(utau)(i))|.

  The difficulty of a case expression is in the uses of identifiers. Without a case expression, all
  identifiers appear in the same execution path. With a case expression, this is no longer the case,
  if some identifier |x| occurs in a branch, and also in another branch, then these two uses of |x| are
  not in the same execution path. For each execution path, we determine how often an identifier is
  used and take the maximum over the execution paths.
  
  Consider the following example:

%%[[wrap=code
  \(x :: (Sub(utau)(1))) -> case (x :: (Sub(utau)(1))) of
                              True   -> (x :: (Sub(utau)(2)))
                              False  -> (x :: (Sub(utau)(3))) && (x :: (Sub(utau)(4)))
%%]

  We know build from the abstract syntax tree an arithmetic expression that combines the individual results
  of the uses of |x|. We start with the first branch. Only one occurrence of |x|, so the arithmetic
  expression is just |(Sub(utau)(2))|. Then the second branch. There are two occurrences of |x| in sequence,
  so the arithmetic expression is |(Sub(utau)(3)) \+/ (Sub(utau)(4))|. We now combine the branches by taking
  the maximum of the two execution paths, denoted by |\-/|: |(Sub(utau)(2)) \-/ ((Sub(utau)(3)) \+/ (Sub(utau)(4)))|.
  The guard of the case expression has only a single occurrence of |x|, resulting in the expression |(Sub(utau)(1)))|.
  Finally, for the entire case expression we combine the guard and the branches. The guard is in the same execution
  path as all the branches, resulting into the arithmetic expression: |(Sub(utau)(1))) \+/ ((Sub(utau)(2)) \-/ ((Sub(utau)(3)) \+/ (Sub(utau)(4))))|.
  By means of the definition site, the arithmetic expression is turned into a constraint:
  |(Sub(utau)(1))) \+/ ((Sub(utau)(2)) \-/ ((Sub(utau)(3)) \+/ (Sub(utau)(4)))) <= (Sub(utau)(0))|.
  
  To summarize: we treat each occurrence of an identifier as being in the same execution path, unless they appear in
  different branches of a case expression. In the presence of a |Let| binding, this approach is somewhat conservative.
  For example:

%%[[wrap=code
  \b ->  let  x = 3
              y = x
              z = x
         in   case b of
                True   -> y
                False  -> z
%%]

  The |y| and |z| are both used once, assuming that the entire expression is used once. So, both occurrences of
  |x| are used once. We assumed by the above approach that these two occurrences of |x| are in the same
  execution path, and will thus add up the reference counts, with the result that |x| is considered shared. But
  if we look at the case-expression, we see that |y| and |z| do not appear together on an execution path. So,
  instead of adding up, it is safe to only take the maximum.

  To get this improvement implemented, we need to determine which bindings do not appear in the same execution
  path. These can be obtained from the syntactical structure of the program, by means of a control flow
  analysis with monotone frameworks~\cite{fill this in}. Each use-site of an identifier is labelled, and we
  determine for each expression a set, where each element in the set is a set of identifier labels that occur
  sequential. To illustrate this, consider the above example again:

%%[[wrap=code
  (\b ->  (  let  x = 3 [12]
                  y = (Sub(x)(1)) [13]
                  z = (Sub(x)(2)) [14]
             in   (  case (Sub(b)(3)) [16] of
                       True   -> (Sub(y)(4)) [17]
                       False  -> (Sub(z)(5)) [18]
                  ) [15]
          ) [11] ) [10]
%%]

  Each use of an identifier is labelled (1-5), as well as each expression (10-17). For each expression we
  find the following sets:

  \begin{tabular}{l||l}
  Expression & Labels \\
  \hline
  10 & $\{ \{\} \}$ \\
  11 & $\{ \{1, 3, 4\}, \{2, 3, 5\} \}$ \\
  12 & $\{ \{\} \}$ \\
  13 & $\{ \{1\} \}$ \\
  14 & $\{ \{2\} \}$ \\
  15 & $\{ \{1, 3, 4\}, \{2, 3, 5\} \}$ \\
  16 & $\{ \{3\} \}$ \\
  17 & $\{ \{1, 4\} \}$ \\
  18 & $\{ \{2, 5\} \}$
  \end{tabular}

  The results for expressions 17 and 18 show that branch 17 and 18 are independent. Result 15 shows that
  label 3 is sequential to either expression, and the results are obtained by a carthesian product between
  $\{3\}$ and $\{ \{1, 4\}, \{2, 5\} \}$. Finally, result 10 shows that sets are removed when none of the
  labels are in scope.

  Unfortunately, the existence of (higher order) functions, make this analysis more complicated. At least,
  if we want to treat each use of a function independently (intra-functional analysis). See
  InsertAuthorNameHere, et al~\cite{fill this in} for more information about this subject.

  With this information, we can determine the addition constraints. We take the result
  set |S| of expression 11 and build the arithmetic expression by taking the maximum (|\-/|) over the groups,
  and the sum (|\+/|) over each occurrence of |x| in such group:

%%[[wrap=code
  toConstraint x s
    =  foldr  (\p q -> sumup p \-/ sumup q) (<=)               -- use maximum between groups
              [s' | s' <- s, any (`isLabelOf` x) s' ]          -- take only a group with x in it
    where
      sumup s
        = foldr1 (\+/) [ typeOf l | l <- s, l `isLabelOf` x ]  -- use addition inside group
%%]

  This approach can be improved further by incorporating the results from reference-counts. For that we
  need to change the initial solution of the control flow analysis. Instead of the empty set, we take a
  set with has the singleton set |{l}| for each use-site occuring in it, labeled |l|. This makes sure
  that we initial assume that there is a constraint |(Sub(utau)(1)) \-/ ... \-/ (Sub(utau)(2)) <= utau|
  between some identifier |x :: utau| with use-sites |(Sub(x)(i)) :: (Sub(utau)(i))|. This initial solution
  properly propagates information about if a value is used zero or one or more times, but does not properly
  propagate if a value is used more than once (since that requires the use of |\+/| operators in the
  constraints). But, as soon as information becomes available of expressions that
  are being used, the control flow analysis will produce more sets, and the constraint then evolves to
  a constraint that properly adds annotations where needed. Essentially, the fixpoint computation in
  the solve process does not only involve the substitution, but also the constraints. This is not
  surprizing, since this already is the case with the instantiation discussed in Section~\ref{fill this in},
  where constraints are duplicated during the fixpoint computation.

  The benefits of the control flow analysis are questionable. The sets can grow large, and the analysis
  can be an intensive job for the compiler, especially in the presence of higher-order functions. So,
  in a practical setup, some conservative approximations to the sets will be needed, but in that case,
  not performing this analysis gives probably equivalent results. However, this is a topic that may
  be interesting to investigate. The improvements to the control flow analysis are nice from a
  technical perspective, but make the matters only more resource-intensive for the compiler, influences
  the graph reduction strategies, and will not be useful in practice.

\section{Records}

  Todo. Don't know if I should spend time on records, since it is not interesting for the reader.

\section{Conclusion}

  Data types are an interesting language feature in terms of what complications they give in order to
  support them. Good abstractions are required in the implementation, otherwise code becomes soon
  so interconnected that it is no longer feasible to grasp. Our approach with constraint gathering,
  constraint graph construction with annotation trees, and constraint solving, provides ways of splitting
  concerns in small enough pieces.

  An important aspect of the approach taken in this chapter is, is that the part that is concerned with
  the analysis itself (constaint solving), is almost not affected. This means that data types do not
  require us to change our way of reasoning about uniqueness typing, and only complicate the analysis on
  a technical level, instead of a conceptual level. Perhaps that the work on data types can be reused
  for other analyses as wel.

  One aspect that we did not cover in this chapter, is how we determine the variance of an annotation,
  and whether or not the values it classifies are parameters or values of a function. We discuss this
  in Chapter~\ref{chapt.Polarity}.


\chapter{Polarity and Under-the-arrow analysis}
\label{chapt.Polarity}

Chapter~\ref{chapt.DataTypes} introduces data types. For annotations on the type constructors of these
data types, we need to know what their variance is and if they occur as function parameter or value. In
this chapter we discuss how to infer this information from the data type definitions and the types in
the program. For that, we reuse the entire analysis that we explained in this thesis, and apply it to
the type language instead of the expression language. Notice that this chapter is not about uniqueness
typing, but about a whole other analysis!

\section{Introduction}

  In this section we introduce the concepts of covariance and below-the-function-arrow on simple types
  without data types.

  An annotation is \emph{covariant} if it only occurs of types that are covariant. Likewise, an annotation
  is \emph{contra-variant} if it only occurs on types that are contra-variant. If it is neither, then it
  is called \emph{variant}. Without data types, a type is either an |Int|, or the function type |tau -> tau|,
  and suppose that we also have tupples |(tau, tau)|. We then define the variance as follows:

%%[[wrap=code
  variance t = var t CoVariant

  var (Sup(Int)(delta)) c         = [(delta, c)]
  var (t1 (Sup(->)(delta)) t2) c  = [(delta, c)] ++ var t1 (neg c) ++ var t2 c
  var (t1 (Sup(,)(delta)) t2) c   = [(delta, c)] ++ var t1 c ++ var t2 c

  neg  CoVariant      = ContraVariant
  neg  ContraVariant  = CoVariant
%%]

  Effectively, this definition means that the variance of a function argument is the inverse of a
  function result. Apply the |variance| function to |(((Annot(Int)(1)) (Annot(->)(4)) (Annot(Int)(2))) (Annot(->)(5)) (Annot(Int)(3)))|
  to discover that |(Delta(3))| and |(Delta(5))| are covariant, that |(Delta(2))| and |(Delta(4))| are contra variant (because of the negation),
  and that |(Delta(1))| is covariant again.

  We give a similar definition for below-the-function arrow. An annotation is \emph{below-the-function-arrow} if
  it only occurs of types that are below-the-function-arrow. Likewise, an annotation
  is \emph{not-below-the-function-arrow} if it only occurs on types that are not-below-the-function-arow. If it is neither, then we
  assume that it is both. Again, we can give a defintion in terms of the simple types:

%%[[wrap=code
  below t = bel t NotBelow
  bel (Sup(Int)(delta)) c         = [(delta, c)]
  bel (t1 (Sup(->)(delta)) t2) c  = [(delta, c)] ++ bel t1 Below ++ bel t2 Below
  bel (t1 (Sup(,)(delta)) t2) c   = [(delta, c)] ++ bel t1 c ++ bel t2 c
%%]

  For the example |(((Annot(Int)(1)) (Annot(->)(4)) (Annot(Int)(2))) (Annot(->)(5)) (Annot(Int)(3)))|, this definition means that
  |(Delta)(1)|, |(Delta(2))|, |(Delta(3))|, and |(Delta(4))| are below the function arrow, and |(Delta(5))| is not.

  But, these are just definitions for a type language without data types. Data types complicate these concepts because it depends
  on the definition of the data type what is done with the type arguments of a type constructor. We demonstrate by an example
  in the following section.

\section{Example}

  Suppose we have a slight alteration of the commonly known |GRose| data type, which we will call |FRose|:

%%[[wrap=code
  data (Dot(FRose)(Delta(2))(Delta(1))()) f a = FBranch (f a ((Dot(FRose)(Delta(1))(Delta(1))()) f a))
  FRose :: forall a . (a -> * -> *) -> a -> *
%%]

  And consider the following types:

%%[[wrap=code
  (Dot(FRose)(Delta(4))(Delta(3))()) (Annot((,))(5)) (Annot(Int)(6))
  (Dot(FRose)(Delta(8))(Delta(7))()) (Annot((->))(9)) (Annot(Int)(10))
%%]

  We now ask ourself the following questions:

  \begin{itemize}
  \item What is the variance of |(Delta(6))| and |(Delta(10))|?
  \item If the type will be expanded indefnitely, will |(Delta(6))| and
        |(Delta(10))| always occur below a |(->)|? Which means that |(Delta(10))|
        always occurs on types of values that are function arguments or results.
  \end{itemize}

  An analysis on the types is required to answer these questions. For |(Delta(6))|
  we see that the type definition represents an right-infinitely deep tupple:

%%[[wrap=code
  ((Annot(Int)(6)), ((Annot(Int)(6)), ((Annot(Int)(6)), ((Annot(Int)(6)), ((Annot(Int)(6)), ... )))))
%%]

  Applying what we know from Section~\ref{fill this in}, we discover that |(Delta(6))| is covariant
  and not below a function arrow.

  The inverse is case for |(Delta(10))|. The type with |(Delta(10))| represents the type of
  a function with infinitely many parameters:

%%[[wrap=code
  ((Annot(Int)(6)) -> ((Annot(Int)(6)) -> ((Annot(Int)(6)) -> ((Annot(Int)(6)) -> ((Annot(Int)(6)) -> ... )))))
%%]

  Applying the techniques of Section~\ref{fill this in} again, and we see that for the variance, each |(Delta(6))|
  is a multiple of two negations away from an other |(Delta(6))|, and that if there would be a final |(Delta(6))|,
  that it is the result of a function, thus all occurrences of |(Delta(6))| are covariant. As is plainfully
  visible in this example, all occurrences of |(Delta(6))| are below a function arrow.
  
  How are we going to infer these results? What we see here is that the analyses work by pushing some known information
  topdown into a definition. This is exactly what we did with the propagation of reference counts. Similary, combining the
  results of each individual use-site to one single definition-site, is what we did with the |\+/| constraint. So, by
  performing the analysis on the type level, instead of the expression level, on kinds instead of types, we can use the
  results of previous chapters to solve the problem of this chapter. For that, we only need different constraint gathering
  rules (although they look a lot like those for uniqueness typing) and a different interpretation of the constraints.


\section{Constraint gathering}

  Constraint gathering is performed on the type level. A type expression does not differ much compared to a normal
  expression, except that there are no lambda abstractions.
  
  Figure~\ref{fill this in} lists the typing rules for constraint gathering on type expressions. We see here that the
  rules are virtually the same as those for normal expressions. At each use of a type constructor, we generate an
  |Inst| constraint. Uses of type variables do not need constraints (similar to lambda-bound identifiers for
  expressions), and at the definition site of variables, we combine the individual results of the variables with a
  |\+/| constraint. Finally, at an application, at type applications, there are two |=>=| constraints generated to
  flow results from parameter to argument, and from type expression result to function result.

\section{Constraint interpretation}

  To solve the constraints, we refer back to Chapter~\ref{fill this in}, and especially Section~\ref{fill this in}. Instead
  of two solver phases, we only need a single solver phase.

  We consider variance first. There is a problem with variance in the sence that some of the propagation constraints are
  not negations, and negations where not represented by the constraint gathering in Section~\ref{fill this in}. But, the
  negations only occur in the constraint set of the function arrow, and thus we only have to put this new constraint
  into the initial constraint set:

%%[[wrap=code
  (->) :: (Annot(*)(1)) -> (Annot(*)(2)) -> (Annot(*)(3)), (Delta(3)) =>= (Delta(2)), (Delta(3)) (Sub(=>=)(/=)) (Delta(1))
  Int  :: (Annot(*)(4))
%%]

  This new constraint has some slight complications on the constraint graph, however, since care has to be taken that
  simplification of an odd number of negation edges, results still into an odd number of negation edges. In principle,
  all the rewrites rules can be reused, but some have to be adjusted slightly in order to guarantee that negation edges
  are properly dealt with.

  The infrastructure remains the same. You may wonder how we deal with variance and below-the-function-arrow when
  converting a constraint with kinds to edges in the constraint graph, since this was a missing feature of the
  infrastructure that required this chapter. In this case, the type language of kinds is simple enough that
  we do not need this analysis, but use a variation on the definitions mentioned in the introduction of this chapter,
  instead.

  So, the only change now is in the interpretation of the constraints. We interpret the constraints to find a
  substitution. A substitution maps an annotation to some analysis result. In case of variance, we use for
  that the following lattice:

%%[[wrap=code
  -- insert a picture with '?' <= '+' <= '+/-', and '?' <= '-' <= '+/-'
%%]

  Then the interpretation is defined as follows:

%%[[wrap=code
variance (a =>= b)
  = a =>= (a `join` b)
variance (a (Sub(=>=)(/=)) b)
  = a =>= (neg a `join` b)
variance (a \+/ b <= c)
  =  let  z = a `join` b `join` c
     in   z \+/ z <= z

join  ?  x  = x
join  x  ?  = x
join  x  y
  |  x == y     = x
  |  otherwise  = +/-
%%]

  For the below-the-function-arrow analysis, no special constraints are needed. But, the initial substitution needs
  to be adapted for a special annotation |delta| that represents below-the-function-arrow. Then the initial constraint
  set for below-the-function-arrow analysis is:

%%[[wrap=code
  (->) :: (Annot(*)(1)) -> (Annot(*)(2)) -> (Annot(*)(3)), delta =>= (Delta(1)), delta =>= (Delta(2))
  Int  :: (Annot(*)(4))
%%]

  The lattice that we use is similar to that of variance:

%%[[wrap=code
  -- insert a picture with '?' <= 'below' <= 'both', and '?' <= 'not-below' <= 'both'
%%]

  Again, we formulate the interpretation function in a familiar way:

%%[[wrap=code
below (a =>= b)
  = a =>= (a `join` b)
below (a \+/ b <= c)
  =  let  z = a `join` b `join` c
     in   z \+/ z <= z

join  ?  x  = x
join  x  ?  = x
join  x  y
  |  x == y     = x
  |  otherwise  = both
%%]

  The result is that the constraints specify the traversal over the data types, and that the interpretation function
  perform the analsis that we did by hand in the introduction of this chapter.


\section{Graph simplification}

  As we noted, the graph simplification can remain largely unaltered. But we can optimize the graphs resulting from
  these analyses more. The reason is that the |\+/| constraint can be rewritten to |=>=| constraints. A constraint
  of the form |a \+/ b <= c| means |a == b == c| in the interpretation, thus we can replace it with |a =>= b =>= c|
  and |c =>= b =>= a|. The graphs for data types are even more sparse than those of expressions, with the result that
  the resulting graphs are very small. This is important, since we have to perform the above analyses for each type
  in the constraints.


\section{Conclusion}

  This chapter shows that the results of previous chapters can not only be used for uniqueness typing, but for
  two `completely different' analyses as well, without much additional work. We did not compare the performance
  of the variance approach to a non-constraint based variance approach, but we think that due to the simplifications
  on the constraint graphs, that our approach is not much slower, although from some perspective, all the work that
  we did for uniqueness typing, may be seem as overkill to solve the variance problem.

%%]

