%%[main

\chapter{Adding algebraic data types}
\label{chapt.DataTypes}

Note: need to look again at pattern matches.
Note: need to say something about which parts of a value are evaluated at the root of the expression (the others should have a lower bound of 0).

We almost have an industry-ready language from a typing point of view. The mayor type system feature that we
lack are algebraic data types. We will encounter pattern matches as the only new concept of this chapter,
all other features are already covered in previous chapters.

Support for data types involves changes at two levels. The expression level needs to deal with pattern matches
and case-expressions. At the type level, the types become more complicated. Types have more annotations, dictated
by the types in an algebraic data type definition. Although algebraic data types influence a lot of components of a computer, the
required changes are surprisingly isolated. The constraint gathering phase need to deal with case expressions and pattern matches.
The solver is unaffected. The only place that is heavily affected, is the conversion
to annotation trees (Section~\ref{sect.ReferenceCountAnnotations}). In order to perform this conversion, it turns out that we
have to perform two analysis on the data type definitions (Chapter~\ref{chapt.Polarity}).

This chapter is organised as follows. We start with an example that illustrate what the problems are to
support the data types. Each of these problems are discussed in a subsequent section in isolation. The
contents of these sections require a fair understanding of the previous chapters.


\section{Example}

The example data type that we take in this chapter, are lists:

%%[[wrap=code
data List a  =  Cons a (List a)
             |  Nil
%%]

The algebraic data type definition dictates how the representation of a |List Int| in memory is (see Figure~\ref{fig.ListRep}). In this case, a
list is a region of memory representing a |Cons| or a |Nil|. In the first case, the area of memory
has two pointers, one to a value of type |a|, and a pointer to the remainder of the list. Each area
of memory has a corresponding type in the algebraic data type definition. The other way around, each type in
a algebraic data type definition, corresponds to zero or more memory regions of a value of that data type.
Similar to Section~\ref{sect.TpAnnConstr}, annotations on the type constructors of the data type, classify
the uniqueness properties of a piece of memory. This example also shows why annotations on the types
are useful: a value can be infinite, but the representation of the type is finite.

With the annotation problem taken care of, the existing infrastructure knows how to deal with
expressions with user-defined data types as types. But, in order to construct and deconstruct such
values we need a few alterations in the expression language, by means of value constructors,
pattern matching, and case-expressions. Value construtors are just functions with a type derived
from the corresponding algebraic data type definition. We already know how to deal with those. But
pattern matches and case-expressions deserve some attention.

The annotations are important in the case of pattern matching. Pattern matching touches the spine of
a value, and if it succeeds, can bring identifiers into scope. Constraints need to be generated
in order to properly flow use-counts into the annotations on the spine of data types. Annotations
for the types of the identifiers introduced by the pattern match, needs to be obtained from
the type of the pattern and the corresponding algebraic data type definition (Section~\ref{sect.PatternMatch}).
For example, if some value has type |(Annot(tau)(0)) === (Annot(List)(1)) (Annot(Int)(2))|, and we perform a successful
pattern match on |Cons (x :: (Annot((Sub(tau)(1)))(3)) (xs :: (Annot((Sub(tau)(2)))(4)))|, then each use
of |x| and |xs| can potentially result in a use of the spine of the value towards the location where
|x| or |xs| is stored, so we generate the constraints: |(Delta(3)) (Sub(=>=)(s)) (Delta(0))| and |(Delta(4)) (Sub(=>=)(s)) (Delta(0))|.
For the strict parts in the pattern, in this case the |Cons| constructor alone (with |(Delta(0))| being the corresponding
annotation), we generate a constraint |delta (Sub(=>=)(s)) (Delta(0))|, where |delta| is then number of uses of the expression containing the
pattern match.

Case expressions are a generalisation of the |if..then..else| expressions of Chapter~\ref{chapt.Parallel}. We are not going to
explain this subject again here: the generalisation is straightforward after we know how to deal with pattern matches.


\section{Exposed annotations}
\label{sect.ExposedAnnotations}

  Data types complicate the annotation process. Consider the following algebraic data type definition:

%%[[wrap=code
  data Tuple = T Int Int
%%]

  Only the |Tuple| type constructor gets an annotation |delta|. But what are the annotations to give to both integers? There are several ways to deal
  with this situation. A straightforward way is to annotate each |Int| with |delta|. That way we treat the components of a data type the same
  as the data type itself. Although allowed, this does not provide the accuracy we aim for. A second way is to ask the programmer for the
  annotations on the components of the type, and assume that the annotation is |(Card(*)(*))| if that is not the case. This is the approach taken by
  Clean~\cite{barendsen96uniqueness}. The third way is to expose the annotations on inner types such that the use-site of a data type can choose
  the annotations~\cite{Wansbrough:PhDThesis}:

%%[[wrap=code
  data (Dot(Tuple)(delta)([(Sub(delta)(1)), (Sub(delta)(2))])) = T (Dot(Int)((Delta(1)))({})) (Dot(Int)((Delta(2)))({}))
%%]

  This way, the use-site can choose if the |Tuple| contains two unique integers, two strict integers, or
  perhaps a unique integer and an arbitrary used integer. The list subscripted on a type constructor we call
  the set of exposed annotations. This list contains all annotations occurring inside the
  algebraic data type definition.

  This approach is slightly complicated in case of (mutual) recursive data types, since what list of
  annotations should we pass to the recursive type constructor? This is a familiar problem, for which there is
  a common solution: in a first pass, pick fresh annotations for type constructors from another
  binding group, and take the empty list for annotations from the same binding group. Collect
  all the exposed annotations for each binding-group. In the second pass, set the exposed annotations
  for type constructors of the same binding-group to the list of the entire binding-group. Now, each
  annotation occurring inside the algebraic data type definition, occurs in the list of exposed annotations of
  the corresponding type constructor.

  For example:

%%[[wrap=code
  data (Dot(InfIntList)(delta)([(Delta(1)),(Delta(2))])) = Cell (Annot(Int)(1)) (Dot(InfIntList)((Delta(2)))([(Delta(1)),(Delta(2))]))
%%]

  But what about named type variables in a algebraic data type definition? The problem with type variables is that the
  structure of a type variable can be substituted to virtually any structure at the use-site of a
  data type, and we cannot give the annotations beforehand. But that is not required either: the annotated
  types passed as arguments to a type constructor, contain exactly the annotations that we did not know yet.
  For example:

%%[[wrap=code
  data (Dot(Embed)(delta)([])) a = Emb a
%%]

  The annotations to choose for |a| come from the use-site. For example, the type |(Dot(Embed)((Delta(1)))([])) (Annot(Int)(2))|
  specifies that a value of this type has |(Delta(1))| as annotation on Emb constructor, and |(Delta(2))| on the integer
  that it contains.

  \subsection{Implementation}

  After obtaining the set of annotations for each data type, the implementation of this approach appears to be straightforward
  after the work that we did for annotation trees in Section~\ref{sect.AnnotationTrees}. When annotating a type constructor in a
  type, pick fresh annotations for the exposed annotations, and when processing a constraints with these type in them,
  construct an annotation tree with the list of annotations as attachment on the corresponding node in the annotation
  tree. The treatment of exposed annotations in the annotation tree is the same as the other annotations in the
  annotation trees. There is a complication, however, since we need to know the variance of annotations and the fact
  if they are below a function arrow or not. These two subjects are more involved in the presence of data types.
  Chapter~\ref{chapt.Polarity} provides a way to obtain the required information about the location of annotations.

  \subsection{Conclusion}

  Thus, this approach is flexible and the changes are fairly isolated. There is a downside to this approach, which is
  that types can contain a lot of annotations this way. We can control this number of annotations by giving up accuracy, for
  example by enforcing that the annotations of a certain data type are never taken freshly. This saves annotations when
  the data type is used a lot, but has as consequence that there is only one uniqueness variant of the data type. All values
  of such a data type then have the same cardinality values, which is less flexible.

\section{Expanded types}
\label{sect.ExpandedTypes}

  The exposed annotations approach has limitations. If we look at a list, then each |Cons| in the list is
  treated the same way. We cannot make a distinction between certain elements of a list. On the other
  hand, the reason for taking a list (which is of varying size), is to treat each element in the same way, so that
  is not a big issue. A bigger limitation is that a type variable can only be used in one fashion. For example:

%%[[wrap=code
  data Tuple a = T a a
%%]

  In this example, both occurrences of |a| will get the same cardinality annotation, since we take the annotations from
  the type passed as |a| to |Tuple|. But in the memory representation of such a data type, each |a| is a separate area
  and thus can have different cardinalities. So, can we change the system such that we can treat occurrences of the
  same type variable differently?

  We can achieve this by using an ad-hoc trick: by unfolding the algebraic data type definitions a couple of times. For
  example, unfolding |Tuple Int| results into the type |<T Int Int>|, and can be annotated with annotations in the
  conventional way.  The value-constructor name does not need an annotation, but is part of the type in order to
  determine of which constructor the fields are an expansion. Type expansion is dangerous for recursive types, since
  the expansion process will not terminate, and the representation can explode quickly, especially for nested
  data types.

  The expansion approach and the exposed-annotations approach can be combined. The exposed-annotations approach can
  be used on any type, and the expansion approach on any type of kind |star|. By default, due to the dangerous nature
  of expansions, we do not perform expansions and require the programmer to specify when an expansion is allowed. Assume
  that |utau| is a type annotated by the exposed-annotations approach. Consider that during a topdown visit a type
  |(Sub(utau)(0))| is encountered, with outermost type constructor |T| and parameters |(Sub(utau)(1))|, ..., |(Sub(utau)(n))|.
  If |(Sub(utau)(0))| is of kind |star| and the occurrence of |T| in |utau| is allowed for expansion, then the definition
  of |T| is unfolded with a freshly annotated version of |(Sub(utau)(i))| at each occurrence of type variable |i|. This
  procedure thus works by performing one-step expansions.

  For example, consider the algebraic data type definition of |List|:

%%[[wrap=code
  data (Dot(List)((Delta(1)))((Delta(2)))) a  =  Cons a ((Dot(List)((Delta(2)))((Delta(2)))) a)
                                              |  Nil
%%]

  Suppose that we annotate the type |List Int|. By means of exposed annotations, we obtain the
  type:

%%[[wrap=code
  (Dot(List)((Delta(1)))((Delta(2)))) (Dot(Int)((Delta(3)))([]))
%%]

  A one step expansion gives:

%%[[wrap=code
<Cons (Dot(Int)((Delta(4)))([])) ((Dot(List)((Delta(2)))((Delta(6)))) (Dot(Int)((Delta(5)))([]))), Nil>
%%]

  Note how the exposed annotation |(Delta(2))| is used on his corresponding occurrence on the
  data type during the expansion. Expanding yet another step further, gives:

%%[[wrap=code
<Cons (Dot(Int)((Delta(4)))([])) <Cons (Dot(Int)((Delta(7)))([])) ((Dot(List)((Delta(6)))((Delta(9)))) (Dot(Int)((Delta(8)))([]))), Nil>, Nil>
%%]

  These one-step expansions are not only used for annotating the data types, but also for expanding a type for a
  certain constructor in pattern matches, and in constructing an equivalent annotation tree structure if one
  type has exposed type, and the other an expanded type. So, in a way, the infrastructure that is needed to support
  expanded types, is already needed for the system itself.

  So, how does a programmer specify when a type may be expanded? We use \emph{path expressions}, which is a
  sequence of constructor names with wildcards. A type constructor |T| is allowed to be expanded, when |T|
  occurs in some expanded type, which as parents |(Sub(T)(1)), ..., (Sub(T)(n))|, and there is a prefix of
  a constructor sequence equal to |(Sub(T)(1)), ..., (Sub(T)(n)), T|. It must explicitly mention |T|, but
  it is allowed to have a wildcard for some of the constructor names.

  A path expression is defined by:

%%[[wrap=code
  path  ::=  T path   -- constructor
        |    *T path  -- wildcard
        |    empty
%%]

  A wildcard stands for any number of constructors not equal to the constructor after the wildcard. For
  example, the sequence |* List * List * List| specifies that a list may be unfolded thrice.

  With expanded types, two things can be accomplished: type constructors of kind |*| can get different
  annotations for each occurrence in the body of a data type, and the head of a data type can be
  annotated differently than the (possibly infinite) tail. This power comes at a price: the types grow
  and so does the number of annotations. Therefore, expanded types are off by default, only to be used
  in special circumstances, specified by the programmer by path expressions.

\section{Pattern matches}
\label{sect.PatternMatch}

  Pattern matches bring identifiers into scope, and force evaluation of a certain part of a value to
  determine if a pattern matches or not.

  The structure of a pattern match dictates how to pattern match on a type to obtain the type of
  the subpatterns. An expanded type (Section~\ref{sect.ExpandedTypes}) can directly be overlayed on top
  of a pattern match. An exposed-annotated type can be overlayed on a pattern match after a
  one-step expansion for each pattern match on a constructor. For example, if we take the one-step
  expansion of Section~\ref{sect.ExpandedTypes} as example, then a match |Cons x xs|, with the type of
  the entire pattern match being |<Cons (Dot(Int)((Delta(4)))([])) ((Dot(List)((Delta(2)))((Delta(6)))) (Dot(Int)((Delta(5)))([]))), Nil>|,
  gives |(Dot(Int)((Delta(4)))([]))| for |x| and |(Dot(List)((Delta(2)))((Delta(6)))) (Dot(Int)((Delta(5)))([]))| for
  |xs|. So, for pattern matches, we the annotated type is pushed inwards, expanding it if needed for
  each match on a constructor.

  A pattern match cannot obtain a component from a value without evaluating the spine of that
  value. The upper and lower bounds of the spine are thus at least the upper and lower bound
  of any component. Define the parent of a pattern to be the match on a constructor closest to the
  pattern. For example, in the match |(Sub(Cons)(1)) x ((Sub(Cons)(2)) y ys)|, the parent of
  both |y| and |ys| is |(Sub(Cons)(2))|, and the pattern of |(Sub(Cons)(2))| and |x| is
  |(Sub(Cons)(1))|. Associated with each variable binding, and each constructor, is the
  type of the value that on which the pattern is applied. Suppose |(Delta(1))| is the
  outermost annotation on a constructor match or variable binding, and |(Delta(2))| is the
  outermost annotation on the type of the value of the corresponding parent. We generate a
  constraint |(Delta(1)) (Sub(=>=)(s)) (Delta(2))| to propagate reference counts. Due to
  transitivity in the coercion constraint, each pattern is connected to the parent, the
  parent of the parent, and so on.

  This approach can be refined a bit, depending on guarantees of the code-generation process.
  If the code generator guarantees that a pattern match is only evaluated once for each binding
  (which is a valid assumption with call-by-name semantics), then for each parent pattern, we can take the
  \emph{minimum} between the reference-count via the conventional way, and the reference-count of the
  expression tied to the pattern match. In other words, suppose that |delta| is the annotation on
  a pattern, |(Sub(delta)(par))| the annotation on the corresponding parent, and |(Sub(delta)(expr))|
  the annotation on the expression tied to the pattern match. Then generate the constraint:
  |delta \-/ (Sub(delta)(expr)) <= |Sub(delta)(par))|.

  The other aspect of pattern matches, strictness, is resolved by propagating reference counts from
  the expression tied to the pattern match to the matches on constructors. If |delta| is the outermost
  annotation of the type of the value on which a \emph{refutable| constructor match is applied, and
  |(Sub(delta)(expr))| is the outermost annotation on the type of the corresponding expression, then
  we generate a constraint |(Sub(delta)(expr)) (Sub(=>=)(s)) delta| to capture the strictness effects
  of a pattern match.

\section{Case expressions}

  Constraint gathering for case expressions is fairly straightforward now we know how to deal with
  pattern matches. For the pattern matches and the subexpressions, we apply the constraint
  gathering rules as discussed before. A freshly annotated type |utau| is invented for the result
  type |tau| of the case-expression. Each branch |i|, with annotated type |(Sub(utau)(i))|, is coerced
  to the result type of the case-expressions by generating the constraint |utau =>= (Sub(utau)(i))|.

  The difficulty of a case expression is in the uses of identifiers. Without a case expression, all
  identifiers appear in the same execution path. With a case expression, this is no longer the case,
  if some identifier |x| occurs in a branch, and also in another branch, then these two uses of |x| are
  not in the same execution path. For each execution path, we determine how often an identifier is
  used and take the maximum over the execution paths.
  
  Consider the following example:

%%[[wrap=code
  \(x :: (Sub(utau)(1))) -> case (x :: (Sub(utau)(1))) of
                              True   -> (x :: (Sub(utau)(2)))
                              False  -> (x :: (Sub(utau)(3))) && (x :: (Sub(utau)(4)))
%%]

  We know build from the abstract syntax tree an arithmetic expression that combines the individual results
  of the uses of |x|. We start with the first branch. Only one occurrence of |x|, so the arithmetic
  expression is just |(Sub(utau)(2))|. Then the second branch. There are two occurrences of |x| in sequence,
  so the arithmetic expression is |(Sub(utau)(3)) \+/ (Sub(utau)(4))|. We now combine the branches by taking
  the maximum of the two execution paths, denoted by |\-/|: |(Sub(utau)(2)) \-/ ((Sub(utau)(3)) \+/ (Sub(utau)(4)))|.
  The guard of the case expression has only a single occurrence of |x|, resulting in the expression |(Sub(utau)(1)))|.
  Finally, for the entire case expression we combine the guard and the branches. The guard is in the same execution
  path as all the branches, resulting into the arithmetic expression: |(Sub(utau)(1))) \+/ ((Sub(utau)(2)) \-/ ((Sub(utau)(3)) \+/ (Sub(utau)(4))))|.
  By means of the definition site, the arithmetic expression is turned into a constraint:
  |(Sub(utau)(1))) \+/ ((Sub(utau)(2)) \-/ ((Sub(utau)(3)) \+/ (Sub(utau)(4)))) <= (Sub(utau)(0))|.
  
  To summarize: we treat each occurrence of an identifier as being in the same execution path, unless they appear in
  different branches of a case expression. In the presence of a |Let| binding, this approach is somewhat conservative.
  For example:

%%[[wrap=code
  \b ->  let  x = 3
              y = x
              z = x
         in   case b of
                True   -> y
                False  -> z
%%]

  The |y| and |z| are both used once, assuming that the entire expression is used once. So, both occurrences of
  |x| are used once. We assumed by the above approach that these two occurrences of |x| are in the same
  execution path, and will thus add up the reference counts, with the result that |x| is considered shared. But
  if we look at the case-expression, we see that |y| and |z| do not appear together on an execution path. So,
  instead of adding up, it is safe to only take the maximum.

  To get this improvement implemented, we need to determine which bindings do not appear in the same execution
  path. These can be obtained from the syntactical structure of the program, by means of an analysis similar
  to control flow analysis~\cite{nnh99}. Each use-site of an identifier is labeled, and we
  determine for each expression a set, where each element in the set is a set of identifier labels that occur
  sequential. To illustrate this, consider the above example again:

%%[[wrap=code
  (\b ->  (  let  x = 3 [12]
                  y = (Sub(x)(1)) [13]
                  z = (Sub(x)(2)) [14]
             in   (  case (Sub(b)(3)) [16] of
                       True   -> (Sub(y)(4)) [17]
                       False  -> (Sub(z)(5)) [18]
                  ) [15]
          ) [11] ) [10]
%%]

  Each use of an identifier is labeled (1-5), as well as each expression (10-17). For each expression we
  find the following sets:

  \begin{tabular}{l||l}
  Expression & Labels \\
  \hline
  10 & $\{ \{\} \}$ \\
  11 & $\{ \{1, 3, 4\}, \{2, 3, 5\} \}$ \\
  12 & $\{ \{\} \}$ \\
  13 & $\{ \{1\} \}$ \\
  14 & $\{ \{2\} \}$ \\
  15 & $\{ \{1, 3, 4\}, \{2, 3, 5\} \}$ \\
  16 & $\{ \{3\} \}$ \\
  17 & $\{ \{1, 4\} \}$ \\
  18 & $\{ \{2, 5\} \}$
  \end{tabular}

  The results for expressions 17 and 18 show that branch 17 and 18 are independent. Result 15 shows that
  label 3 is sequential to either expression, and the results are obtained by a Cartesian product between
  $\{3\}$ and $\{ \{1, 4\}, \{2, 5\} \}$. Finally, result 10 shows that sets are removed when none of the
  labels are in scope.

  Unfortunately, the existence of (higher order) functions, make this analysis more complicated. See
  Flemming Nielson, et all~\cite{nnh99}, for more information about this subject.

  With this information, we can determine the addition constraints. We take the result
  set |S| of expression 11 and build the arithmetic expression by taking the maximum (|\-/|) over the groups,
  and the sum (|\+/|) over each occurrence of |x| in such group:

%%[[wrap=code
  toConstraint x s
    =  foldr  (\p q -> sumup p \-/ sumup q) (<=)               -- use maximum between groups
              [s' | s' <- s, any (`isLabelOf` x) s' ]          -- take only a group with x in it
    where
      sumup s
        = foldr1 (\+/) [ typeOf l | l <- s, l `isLabelOf` x ]  -- use addition inside group
%%]

  This approach can be improved further by incorporating the results from reference-counts. For that we
  need to change the initial solution of the control flow analysis. Instead of the empty set, we take a
  set with has the singleton set |{l}| for each use-site occurring in it, labeled |l|. This makes sure
  that we initial assume that there is a constraint |(Sub(utau)(1)) \-/ ... \-/ (Sub(utau)(2)) <= utau|
  between some identifier |x :: utau| with use-sites |(Sub(x)(i)) :: (Sub(utau)(i))|. This initial solution
  properly propagates information about if a value is used zero or one or more times, but does not properly
  propagate if a value is used more than once (since that requires the use of |\+/| operators in the
  constraints). But, as soon as information becomes available of expressions that
  are being used, the control flow analysis will produce more sets, and the constraint then evolves to
  a constraint that properly adds annotations where needed. Essentially, the fixpoint computation in
  the solve process does not only involve the substitution, but also the constraints. This is not
  surprising, since this already is the case with the instantiation discussed in Section~\ref{sect.Instantiation},
  where constraints are duplicated during the fixpoint computation.

  The benefits of the control flow analysis are questionable. The sets can grow large, and the analysis
  can be an intensive job for the compiler, especially in the presence of higher-order functions. So,
  in a practical setup, some conservative approximations to the sets will be needed, but in that case,
  not performing this analysis gives probably equivalent results. However, this is a topic that may
  be interesting to investigate. The improvements to the control flow analysis are nice from a
  technical perspective, but make the matters only more resource-intensive for the compiler, influences
  the graph reduction strategies, and will not be useful in practice.

\section{Records}

  Todo. Don't know if I should spend time on records, since it is not interesting for the reader.

\section{Conclusion}

  Data types are an interesting language feature in terms of what complications they give in order to
  support them. Good abstractions are required in the implementation, otherwise code becomes soon
  so interconnected that it is no longer feasible to grasp. Our approach with constraint gathering,
  constraint graph construction with annotation trees, and constraint solving, provides ways of splitting
  concerns in small enough pieces.

  An important aspect of the approach taken in this chapter is, is that the part that is concerned with
  the analysis itself (constraint solving), is almost not affected. This means that data types do not
  require us to change our way of reasoning about uniqueness typing, and only complicate the analysis on
  a technical level, instead of a conceptual level. Perhaps that the work on data types can be reused
  for other analyses as well.

  One aspect that we did not cover in this chapter, is how we determine the variance of an annotation,
  and whether or not the values it classifies are parameters or values of a function. We discuss this
  in Chapter~\ref{chapt.Polarity}.


\chapter{Polarity and Under-the-arrow analysis}
\label{chapt.Polarity}

Chapter~\ref{chapt.DataTypes} introduces data types. For annotations on the type constructors of these
data types, we need to know what their variance is and if they occur as function parameter or value. In
this chapter we discuss how to infer this information from the data type definitions and the types in
the program. For that, we reuse the entire analysis that we explained in this thesis, and apply it to
the type language instead of the expression language. Notice that this chapter is not about uniqueness
typing, but about a whole other analysis!

\section{Introduction}
\label{sect.VarIntro}

  In this section we introduce the concepts of covariance and below-the-function-arrow on simple types
  without data types.

  An annotation is \emph{covariant} if it only occurs on type constructors that are covariant. Likewise, an annotation
  is \emph{contra-variant} if it only occurs on type constructors that are contra-variant. If it is neither, then it
  is called \emph{variant}. Without data types, a type is either an |Int|, or the function type |tau -> tau|.
  Suppose that we also have tuples |(tau, tau)|. We then define the variance as follows:

%%[[wrap=code
  variance t = var t CoVariant

  var (Sup(Int)(delta)) c         = [(delta, c)]
  var (t1 (Sup(->)(delta)) t2) c  = [(delta, c)] ++ var t1 (neg c) ++ var t2 c
  var (t1 (Sup(,)(delta)) t2) c   = [(delta, c)] ++ var t1 c ++ var t2 c

  neg  CoVariant      = ContraVariant
  neg  ContraVariant  = CoVariant
%%]

  Effectively, this definition means that the variance of a function argument is the inverse of a
  function result. Apply the |variance| function to |(((Annot(Int)(1)) (Annot(->)(4)) (Annot(Int)(2))) (Annot(->)(5)) (Annot(Int)(3)))|
  to discover that |(Delta(3))| and |(Delta(5))| are covariant, that |(Delta(2))| and |(Delta(4))| are contra variant (because of the negation),
  and that |(Delta(1))| is covariant again.

  We give a similar definition for below-the-function arrow. An annotation is \emph{below-the-function-arrow} if
  it only occurs on type constructors that are below-the-function-arrow. Likewise, an annotation
  is \emph{not-below-the-function-arrow} if it only occurs on type constructors that are not-below-the-function-arrow. If it is neither, then we
  assume that it is both. Again, we can give a definition in terms of types without algebraic data types:

%%[[wrap=code
  below t = bel t NotBelow
  bel (Sup(Int)(delta)) c         = [(delta, c)]
  bel (t1 (Sup(->)(delta)) t2) c  = [(delta, c)] ++ bel t1 Below ++ bel t2 Below
  bel (t1 (Sup(,)(delta)) t2) c   = [(delta, c)] ++ bel t1 c ++ bel t2 c
%%]

  For the example |(((Annot(Int)(1)) (Annot(->)(4)) (Annot(Int)(2))) (Annot(->)(5)) (Annot(Int)(3)))|, this definition means that
  |(Delta)(1)|, |(Delta(2))|, |(Delta(3))|, and |(Delta(4))| are below the function arrow, and |(Delta(5))| is not.

  But, these are just definitions for a type language without algebraic data types. Algebraic data types complicate these concepts because it depends
  on the definition of a data type what is done with the type arguments of a type constructor. We demonstrate this by an example
  in the following section.

\section{Example}

  Suppose we have a slight alteration of the commonly known |GRose| data type, which we will call |FRose|:

%%[[wrap=code
  data (Dot(FRose)(Delta(2))(Delta(1))) f a = FBranch (f a ((Dot(FRose)(Delta(1))(Delta(1))) f a))
  FRose :: forall a . (a -> * -> *) -> a -> *
%%]

  And consider the following types:

%%[[wrap=code
  (Dot(FRose)(Delta(4))(Delta(3))) (Annot((,))(5)) (Annot(Int)(6))
  (Dot(FRose)(Delta(8))(Delta(7))) (Annot((->))(9)) (Annot(Int)(10))
%%]

  We now ask ourself the following questions:

  \begin{itemize}
  \item What is the variance of |(Delta(6))| and |(Delta(10))|?
  \item If the type will be expanded indefinitely, will |(Delta(6))| and
        |(Delta(10))| always occur below a |(->)|? If that is the case, then |(Delta(10))|
        always occurs on type constructors of types of values that are function arguments or results.
  \end{itemize}

  An analysis on the types is required to answer these questions. For |(Delta(6))|
  we see that the type definition represents an right-infinitely deep tuple:

%%[[wrap=code
  ((Annot(Int)(6)), ((Annot(Int)(6)), ((Annot(Int)(6)), ((Annot(Int)(6)), ((Annot(Int)(6)), ... )))))
%%]

  Applying the variance rules given in Section~\ref{sect.VarIntro} results in |(Delta(6))| in being covariant
  and not below a function arrow.

  The inverse is case for |(Delta(10))|. The type with |(Delta(10))| represents the type of
  a function with infinitely many parameters:

%%[[wrap=code
  ((Annot(Int)(6)) -> ((Annot(Int)(6)) -> ((Annot(Int)(6)) -> ((Annot(Int)(6)) -> ((Annot(Int)(6)) -> ... )))))
%%]

  Applying the techniques of Section~\ref{sect.VarIntro} again, we see that for the variance, each |(Delta(6))|
  is a multiple of two negations away from an other |(Delta(6))|, and that if there would be a final |(Delta(6))|,
  that it is the result of a function, thus all occurrences of |(Delta(6))| are covariant. As is visible in this
  example, all occurrences of |(Delta(6))| are below a function arrow.
  
  How are we going to infer these results? What we see here is that the analyses work by pushing some known information
  topdown into a definition. This is exactly what we did with the propagation of the upper bounds. Similarly, combining the
  results of each individual use-site to one single definition-site, is what we did with the aggregation constraint. So, by
  performing the analysis on the type level, instead of the expression level, on kinds instead of types, we can use the
  results of previous chapters to solve the problem of this chapter. For that, we only need different constraint gathering
  rules (although they look a lot like those for uniqueness typing) and a different interpretation in the solver.


\section{Constraint gathering}

  Constraint gathering is performed on the type level. A type expression does not differ much compared to a normal
  expression, except that there are no lambda abstractions. Instead of lambda abstractions, there are algebraic algebraic data type
  definitions in the declarations of a Let.
  
  The type of a type is called a kind, denoted by |kappa| (see Section~\ref{sect.PolymorphicKinds}). We allow
  a kind to be annotated, similarly to a type. An annotated kind is denoted by |ukappa|, following the same conventions
  as for types in Section~\ref{sect.TpAnnConstr}.

  Figure~\ref{RulerUniquenessExamples.DC.tp.base} lists the typing rules for constraint gathering on type expressions. We
  omit the cases for extensible records and expanded types, these cases clutter up the type rules, but do nothing more than
  collect the constraints for the type expressions occurring in them.
  
  \rulerCmdUse{RulerUniquenessExamples.DC.tp.base}

  Figure~\ref{RulerUniquenessExamples.UX.expr.base} has a lot of resemblance with Figure~\ref{RulerUniquenessExamples.DC.tp.base}.
  At each use of a type constructor, we generate an
  |Inst| constraint. Uses of type variables do not need constraints, similar to lambda-bound identifiers for
  expressions. Finally, at an application, at type applications, there are two coercion constraints generated to
  flow results from parameter to argument, and from type expression result to function result. This way, we connect
  the annotations on the kinds from the root of the type expression to the leaves.

  How we treat annotations that occur in the type, we make explicit with a separate case for annotations on a
  type constructor. This is a subtle rule in the sense that it connects two annotations that live in different
  worlds: one annotation lives in the kind world and one annotation lives in the type world. An annotated type constructor is encoded as the alternative |Ann| just above the alternative
  |Con|. For example |Ann delta (Con "Int" [])| represents |(Dot(Int)([])(delta))|. In the type rule, we connect
  the annotation on the type constructor to the topmost annotation on the annotated kind of the type constructor.
  We want to know the results of the analysis for the annotations occurring on types, but the analysis only finds
  results for the annotated kinds. With the constraint generated for the annotations, the results of kinds are
  propagated to the actual annotations. From an implementation point of view, this approach makes sure that
  a algebraic data type definition collects proper constraints for exposed annotations.

  We omit the type rules for algebraic data type definitions and leave it with a textual description. For a algebraic data type definition,
  the constraints for al the types in the constructors are collected. The environment is enlarged with annotated kinds
  of the type variables, derived by means of pattern matching from the annotated kind of the data type. An aggregation
  constraint is generated for each type variable, that combines each use-site of the type variable. These constraints
  are then collected in binding-groups and passed to the solver. Types of an expression outside an algebraic data type
  declaration, each get their own binding group.


\section{Constraint interpretation}

  To solve the constraints, we refer back to Chapter~\ref{chapt.Polyvariant}, and especially Section~\ref{sect.TheInferencer}. Instead
  of two solver phases, we only need a single solver phase.

  We consider variance first. There is a problem with variance in the sense that some of the propagation constraints are
  not negations, and negations are not represented by the constraint gathering in Section~\ref{Sect.ConstraintGathering}. But, the
  negations only occur in the constraint set of the function arrow, and thus we only have to put this new constraint
  into the initial constraint set:

%%[[wrap=code
  (->) :: (Annot(*)(1)) -> (Annot(*)(2)) -> (Annot(*)(3)), (Delta(3)) =>= (Delta(2)), (Delta(3)) (Sub(=>=)(/=)) (Delta(1))
  Int  :: (Annot(*)(4))
%%]

  This new constraint has some slight complications on the constraint graph, however, since care has to be taken that
  simplification of an odd number of negation edges, results still into an odd number of negation edges. However, this
  is not much more involving than determining soft paths, so it is only a minor complication, especially because we
  can eliminate hyper edges for the analysis in this chapter entirely (Section~\ref{sect.vargraphsimplification}).

  The infrastructure remains the same. This may raise the question how we deal with variance and below-the-function-arrow
  when converting the constraints of this chapter to constraint graphs. We do not need an exstensive analysis for this:
  the kind language is simple enough that we can obtain variance information with a variation of the functions given
  in the introduction of this chapter. And belowness we fix at |NotBelow| as the concept of belowness is not important
  for the analysis on algebraic data types.

  So, the only big change is in the interpretation of the constraints. We interpret the constraints to find a
  substitution. A substitution maps an annotation to some analysis result. In case of variance, we use for
  this the following lattice |leqV|:

%%[[wrap=code
  ?  leqV  +
  ?  leqV  -
  +  leqV  +/-
  -  leqV  +/-
%%]

  Then the interpretation is defined as follows:

%%[[wrap=code
variance (a =>= b)
  = a =>= (a `(Sub(join)(leqV))` b)
variance (a (Sub(=>=)(/=)) b)
  = a =>= (neg a `(Sub(join)(leqV))` b)
variance ((Sub(a)(1)) \*/ (Sub(a)(n)) <= a)
  =  let  z = (Sub(a)(1)) `(Sub(join)(leqV))` ... `(Sub(join)(leqV))` (Sub(a)(n)) `(Sub(join)(leqV))` a
     in   z \*/ ... \*/ z <= z

neg +  = -
neg -  = +
neg x  = x
%%]

  For the below-the-function-arrow analysis, no special constraints are needed. But, the initial substitution needs
  to be adapted for a special annotation |delta| that represents that it is below the arrow. Then the initial constraint
  set for below-the-function-arrow analysis is:

%%[[wrap=code
  (->) :: (Annot(*)(1)) -> (Annot(*)(2)) -> (Annot(*)(3)), delta =>= (Delta(1)), delta =>= (Delta(2))
  Int  :: (Annot(*)(4))
%%]

  The lattice |leqBL| that we use is similar to that of variance:

%%[[wrap=code
  ?          leqBL  below
  ?          leqBL  not_below
  below      leqBL  both
  not_below  leqBL  both
%%]

  Again, we formulate the interpretation function in a familiar way:

%%[[wrap=code
belowSolveF (a =>= b)
  = a =>= (a `(Sub(join)(leqBL))` b)
belowSolveF ((Sub(a)(1)) \*/ ... \*/ (Sub(a)(n)) <= a)
  =  let  z = (Sub(a)(1)) `(Sub(join)(leqBL))` ... `(Sub(join)(leqBL))` (Sub(a)(n)) `(Sub(join)(leqBL))` a
     in   z \*/ ... \*/ z <= z
%%]

  The result is that the constraints specify the traversal over the data types, and that the interpretation function
  perform the analysis that we did by hand in the introduction of this chapter.


\section{Graph simplification}
\label{sect.vargraphsimplification}

  As we mentioned before, the graph simplification can remain largely unaltered. However, we can convert an aggregation
  constraint for these analysis into coercion constraints. This has the advantage that the graph reducer does not need
  to deal with aggregation constraints. A constraint of the form |a \*/ b <= c| means |a == b == c| in the interpretation,
  thus we can replace it with |a =>= b =>= c| and |c =>= b =>= a|. The graphs for data types are even more sparse than those
  of expressions, which means that the resulting graphs are very small. This is important, because the above analysis is applied
  to all types occurring in constraints.

\section{Conclusion}

  This chapter shows that the results of previous chapters can not only be used for uniqueness typing, but for
  two `completely different' analyses as well, without much additional work. We did not compare the performance
  of the variance approach to a non-constraint based variance approach, but we think that due to the simplifications
  on the constraint graphs, that our approach is not much slower than a direct implementation. From some
  perspective, all the work that we did for uniqueness typing may seem to be a bit overkill to solve 'just' the variance problem.
  On the other hand, the code of the analysis can be reused substantially, which has it charms in separation of
  concerns, documentation, and understanding.

%%]
