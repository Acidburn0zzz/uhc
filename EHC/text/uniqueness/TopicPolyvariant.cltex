%%[main

\chapter{Polyvariant analysis with monomorphic bindings}
\label{chapt.Polyvariant}

In the previous chapter, we generated constrains given an annotated program, and called the annotations valid if and only if
the constraints are satisfied. In this topic, we take a closer look at the constraints. We take a validly typed, but not
annotated, program and decorate the type constructors with \emph{uniqueness variables} |delta| instead of concrete
annotations. Then we generate the set of constraints, which now consists of variables instead of concrete values. A solution
is an assignment of concrete values to the annotations such that the constraints are satisfied. The constraints thus specify
a whole family of solutions. In this chapter, we discuss a technique that infers \emph{the minimal} solution to the
constraints.

To make this chapter slightly more challenging, we add a monomorphic and non-recursive |let| to the expression language defined
in Chapter~\ref{chapt.NoBindings}. With the existence of uniqueness variables, we make the type system polyvariant, which
means that we infer a most general uniqueness typing for a binding, and allow different instantiations for each use of an
identifier.

This chapter is organized as follows. We illustrate by an example the concepts of uniqueness variables, and polyvariance.
Then we discuss the addition of bindings to the expression language, and consequently the concept of binding-groups and
instantiation~\ref{fill this in}. Then we go into the concept of the type inference process~\ref{fill this in}.
The constraints play an important role here, but we go to yet another representation: the constraints are rewritten to
a graph-representation on which we can perform graph rewriting~\ref{fill this in} to simplify the constraints.


\section{Example}

A type can now have uniqueness variables as annotations. Reference-count annotations do not exist anymore, but are used
internally by the constraint solver. Annotated types now look like:

%%[[wrap=code
f  ::  forall (Delta(1))                      .  (Annot(Int)(1))
g  ::  forall (Delta(1))                      .  (Annot(Int)(1)),  1 =>= (Delta(1))
h  ::  forall (Delta(2))                      .  (Annot(Int)(2)) (Annot(->)(2)) (Annot(Int)(2))
j  ::  forall (Delta(3))(Delta(4))(Delta(5))  .  (Annot(Int)(4)) (Annot(->)(3)) (Annot(Int)(5)),  (Delta(5)) =>= (Delta(4)), (Delta(5)) =>= (Delta(3))
%%]

In this case, |f| can be instantiated to |(Sup(Int)(0))|, |(Sup(Int)(1))|, or |(Sup(Int)(*))|. For |g|, only
|(Sup(Int)(1))|, and |(Sup(Int)(*))|, are allowed, since |(Sup(Int)(0))| does not pass the constraint. The annotated
type of |h| demands that the annotations on the argument, value and function are exactly the same. The annotated type
of |j| allows different annotations on argument, value and function, but they have to satisfy the given constraints.

Now suppose we have the following program:

%%[[wrap=code
let  id :: Int -> Int
     id = \x -> x

     y :: Int
     y = 1 + y
in   (Sub(id)(a)) 3 + (Sub(id)(b)) y
%%]

Decorating the type constructors with annotations, gives us:

%%[[wrap=code
id :: (Annot(Int)(2)) (Annot(->)(1)) (Annot(Int)(3))
y  :: (Annot(Int)(4))
%%]

These annotations are constrained as follows\footnote{This is a simplified representation of the actual constrained discussed in the previous chapter.}:

%%[[wrap=code
(Delta(1)) === *
(Delta(4)) === *
(Delta(3)) === (Delta(2))
1 =>= (Delta(3))
%%]

Each use of the |id| function can lead to a different instantiation of the uniqueness variables, as long as the constrains are satisfied. In this
case:

%%[[wrap=code
(Sub(id)(a))  ::  (Annot(Int)(6)) (Annot(->)(5)) (Annot(Int)(7))
(Sub(id)(b))  ::  (Annot(Int)(9)) (Annot(->)(8)) (Annot(Int)(10))
%%]

Of which the following assignment of uniqueness values to annotations is the minimal solution:

%%[[wrap=code
(Delta(5))   ===  *
(Delta(6))   ===  1
(Delta(7))   ===  1
(Delta(8))   ===  *
(Delta(9))   ===  *
(Delta(10))  ===  *
%%]

The constraints we generate for a program specify
a whole family of solutions. Among this family is the solution that sets each uniqueness variable to |*|. But in general, more
specific solutions are possible. There is only one least solution to the constraints, and we show in Section~\ref{fill this in}
how to obtain it. Although we can obtain this least solution, keep in mind that this is the least solution to the constraints.
The constraints are a conservative specification of the original uniqueness typing problem, and there are uniqueness typings
of a program that are valid, but that do not satisfy the constraints.


\section{Uniqueness variables}

In contrast to previous chapter where the type constructors are annotated by a reference-count annotation and a
uniqueness annotation, we now only annotate the type constructors with a fresh uniqueness variable. The example
in Section~\ref{fill this in} showed several types annotated with such uniqueness variables.

Consider the following expression with types at every place:

%%[[wrap=code
let  (f :: Int -> Int) = (\x :: Int -> (x :: Int) + (1 :: Int) :: Int) :: Int -> Int
in   (f :: Int -> Int) (1 :: Int) :: Int
%%]

An annotation strategy is to annotate all of these types with fresh uniqueness variables. For example:

%%[[wrap=code
let  (f :: (Annot(Int)(1)) (Annot(->)(2)) (Annot(Int)(3))) = (\(x :: (Annot(Int)(4))) -> (x :: (Annot(Int)(5))) + (1 :: (Annot(Int)(6))) :: (Annot(Int)(7))) :: (Annot(Int)(8)) (Annot(->)(9)) (Annot(Int)(10))
in   (f :: (Annot(Int)(11)) (Annot(->)(12)) (Annot(Int)(13))) (1 :: (Annot(Int)(14))) :: (Annot(Int)(15))
%%]

As demonstrated in Chapter~\ref{fill this in}, the constraints between annotations make sure that the uniqueness
variables are properly connected to each other.

The above approach allows uniqueness coercions everywhere in the abstract syntax tree. But there is redundancy in
the places where coercions can occur. In the binding |x = f 3|, it is possible to coerce the result of |f| at the
function application, but also at the place where the result of |f| is bound to |x|, or even both.
Coercions lead to additional code insertion in the program, depending on the code generator. We therefore want to
limit the number of coercions that can be inserted, and for clarity, restrict it to clearly defined places.

We can restrict the places where uniqueness coercions are inserted to the argument of function applications and
the right hand side of a binding to an identifier, without reducing the quality of the uniqueness inference,
that is, without inferring worse uniqueness types. At the other places, an annotated type can be constructed by
using the uniqueness variables occurring on type constructors in types of the subexpressions. For the
above example:

%%[[wrap=code
let  (f :: (Annot(Int)(1)) (Annot(->)(2)) (Annot(Int)(3))) = (\(x :: (Annot(Int)(4))) -> (x :: (Annot(Int)(5))) + (1 :: (Annot(Int)(6))) :: (Annot(Int)(7))) :: (Annot(Int)(4)) (Annot(->)(9)) (Annot(Int)(7))
in   (f :: (Annot(Int)(11)) (Annot(->)(12)) (Annot(Int)(13))) (1 :: (Annot(Int)(14))) :: (Annot(Int)(13))
%%]

This strategy essentially replaces some |=>=| constraints with unification on uniqueness types. Reducing constraints
and annotations results into better performance of the type inferencer, which is important, since the
example already shows that the number of annotations grows rapidly. To scale up to typing big real-life
programs, a uniqueness type inferencer can decide to reduce accuracy for certain unimportant pieces of a program,
by limiting the coercions and propagate existing uniqueness variables even more, to reduce the number of
annotations severely, and thus preventing choking up on the number of annotations (Section~\ref{fill this in - future work}).
Limiting coercions can also positively impact runtime performance, depending on the additional code that is inserted for it,
and what the code-improvements are up to the point of the coercion (Section~\ref{fill this in - code generation}).

Take a look at the example again. The types at the use-site of an identifier are freshly annotated, but this has nothing to
do with coercions. The reason for freshly annotating the use-sites of an identifier is because we treat each use of an
identifier independently, and combine the results at the definition site with the |\+/| constraint.

Figure~\ref{fill this in} lists the type rules for the uniqueness type system with uniqueness inference. The constraint
generation remained virtually the same, the notable changes are the places where a normal type is freshly annotated to
an annotated type with uniqueness variables. The result of the inference process are annotated types for each expression,
and a substitution, mapping a uniqueness result to each uniqueness variable.


\section{Constraints}

The constraint gathering rules of Section~\ref{fill this in} do not need much change in order to support
uniqueness inference. The major difference, aside from constraints between uniqueness variables, is in
the typing of the addition. To type the addition, we need a softer |=>=| constraint that works on
reference-counts only. This soft version is denoted by |(Sub(=>=)(s))|. The default version, we either
denote by |=>=|, or by |(Sub(=>=)(h))|.


\section{The inferencer}

The question that now remains is: how to obtain the substitution that maps a concrete uniqueness value to a uniqueness
variable. Since we have a finite number of uniqueness variables, and a finite number of uniqueness values (|0|, |1|, and |*|),
an obvious approach is to iterate through all combinations, and pick from the combinations that satisfy the constraints,
the least solution. This approach works, but is not feasible in practice.

A better approach is creating an initial substitution of each uniqueness variable to |0|, and then for each constraint
minimally increasing this substitution until the constraint holds, repeatedly until the substitution does not change
anymore and a least fixpoint is reached. There is only one such least fixpoint, and it is the optimal solution to the
constraint set. We always end up in this least fixpoint, since the substitution is finite, and we can only increase the
substitution until each uniqueness variable is mapped to |*|. This  approach is fast: when the constraints are
topologically ordered, with the left-hand side(s) as dependency on the right-hand-side, and a special treatment
for constraints that form a cycle, at most 5 iterations are required: 3+1 iterations in the worst case to get |*| at every place, and one
iteration to discover that nothing changed. With an implementation by means of a worklist algorithm, it takes even less
time in practice, due to several reasons, including that the constraints represented as a graph (Section~\ref{fill this in})
contain relatively few and big cycles, and are sparse.

But, in practice, we run into the same problem as in the previous chapter: the uniqueness values alone do not tell
enough to infer a valid solution. The problem lies with the |\+/| constraint. It serves two purposes: adding up
individual uses, and making sure the uniqueness values of the involved uniqueness variables are the same. Consider
the following example:

%%[[wrap=code
let  const :: Int -> Int -> Int
     const = \y z -> y
 in  \x -> (Sub(x)(1)) + const 1 (Sub(x)(2))
%%]

Gathering constraints for this program will result into a constraint |(Delta(Sub(x)(1))) \+/ (Delta(Sub(x)(2))) <= (Delta(x))|.
The first purpose of the constraint makes sure that uses of |(Sub(x)(1))| and |(Sub(x)(2))| are added together
and stored in |(Delta(x))|. The second use of the constraint is to make sure that (Delta(Sub(x)(1))), (Delta(Sub(x)(2))),
and |(Delta(x))| are mapped to the same uniqueness values. Performing these tasks at the same time results into
a circular problem: (Sub(x)(1)) is only used once, and |(Sub(x)(2))| is not used. Then |x| is considered unique.
But now (Delta(Sub(x)(1))), and (Delta(Sub(x)(2))) will both get mapped  to |1|. But now the |\+/| constraint has to be satisfied
again, which causes |x| to be incorrectly considered |*|! This is the reason why we needed reference-count annotations in
Section~\ref{fill this in}: to make sure that both tasks of the |\+/| constraint are independent.

We apply the same trick here by phasing the solve process. In the first phase, we only consider the first use of
the |\+/| constraint, effectively calculating reference-counts instead of uniqueness values. Then we take the
results as initial solution for the second phase, where we only consider the second use of the |\+/| constraint,
thereby effectively calculating the uniqueness values and propagating them:

%%[[wrap=code
initialSubst  = [ (delta, 0) | delta <- allAnnotations ]
finalSubst    = worklistFix uniquenessPropagate cs (worklistFix referenceCount cs initialSubst)
%%]

The |worklistFix| function performs the fixpoint iteration. It takes a constraint from the set of
constraints |cs|, applies the current substitution to it, and asks the |uniquenessPropagate| or
|referenceCount| function to increase the values to satisfy the constraints.

The |referenceCount| function performs the addition for the |\+/| constraint, and just propagates
the reference-count to the right-hand side for the |=>=| constraint:

%%[[wrap=code
referenceCount (a (Sub(=>=)(delta)) b)
  | a == 2     = a (Sub(=>=)(delta)) (b `max` 1)  -- coercion
  | otherwise  = a (Sub(=>=)(delta)) (a `max` b)
referenceCount (a \+/ b <= c)
  =  let  z = c `max` (a + b)
     in   a \+/ b <= z
%%]

The |uniquenessPropagate| function uses the |\+/| constraint to set the uniqueness of all uses
of an identifier to the same uniqueness value, and uses the |=>=| to propagate changes through
the uniqueness variables:

%%[[wrap=code
uniquenessPropagate (a (Sub(=>=)(h)) b)
  = (a `max` b) (Sub(=>=)(h) b
uniquenessPropagate (a (Sub(=>=)(s)) b)  -- soft version, no effect on uniqueness substitution
  = a (Sub(=>=)(s)) b
uniquenessPropagate (a \+/ b <= c)
  =  let  z = a `max` b `max` c
     in   z \+/ z <= z
%%]

Technically speaking, the |=>=| constraint works in the reverse direction for uniqueness than for
reference counting. Can this undo what is done at the reference-count stage? The answer is no: the
uniqueness propagation phase can only increase the substitution.

Another technical detail is that the data type representing reference counts and uniqueness values
is considered the same, with |*| also representing |2|, and the other way around.

To summarize: in Chapter~\ref{fill this in} we showed how to gather the constraints. In this chapter,
and this section specifically, we showed how to infer the uniqueness types given the constraints,
using a fixpoint iteration approach.

We are now going to make life a bit interesting by supporting a non-recursive Let. This will complicate
the inferencer, especially since we are paving a way to support a polymorphic and recursive Let for
Chapter~\ref{fill this in} and Chapter~\ref{fill this in}. A gentle warning: make sure you have a feeling
of the inference process up to this point, otherwise subsequent chapters will be difficult to understand!



\section{Adding a non-recursive Let}

The remainder of this chapter is about adding a non-recursive Let. We will show that we can deal with
this language construct by means of constraint duplication. We can handle this constraint duplication
during the constraint gathering phase, but also during the uniqueness inference phase. There are several
reasons to deal with constraint duplication during the inference phase. Some of the reasons will only
become clear in Chapter~\ref{fill this in} and Chapter~\ref{fill this in} about adding a polymorphic and recursive Let, but we can
already mention an advantage in this chapter. Constraint duplication leads to an explosion in the
number of constraints, but we keep this constraint set manageable by applying several simplification
strategies to it (Section~\ref{fill this in}), which benefits from uniqueness information that is produced during the
uniqueness inference phase.

Before we go into the details and complications of the uniqueness inference process for the Let, we
first discuss how to deal with it during the constraint gathering phase. Assume we have the following Let-construct:

%%[[wrap=code
let  id = def_expr
in   body_expr
%%]

Technically speaking, there is no use for allowing more than one binding to occur in a Let, since the
Let is non-recursive and monomorphic as well. However, in preparation for subsequent chapters, we assume there
can be many bindings inside the same Let, although we often only use one binding in each Let for type rules and
examples.

For the conventional type system, a non-recursive, monomorphic Let is a trivial addition to the type system: infer a type |tau|
for |def_expr| with the same environment |Gamma| as the entire expression, then use |tau| for each occurrence of |id|
by adding it to |Gamma| for |body_expr|. Since the Let is non-recursive, the environment for |def_expr| does not need knowledge
about |id|, but that will change once we want to support a recursive Let in chapter~\ref{fill this in}. Figure~\ref{fill this in}
shows the type rule in a more formal fashion.

...type rule for Let...

We can apply the same strategy for the uniqueness type system. A set of constraints $S$ is inferred for |expr_def|. For each use
of |id|, $S$ is copied as the constraint set of the identifier. To show why this approach works, we need the concept of
\emph{referential transparency}. Referential transparency means that an identifier can be replaced by its definition, without
changing the result of the program. A pure functional programming language has this property by definition~\cite{fill this in}.

Suppose the occurrences |1..n| of |id| in |body_expr| are replaced by |def_expr|, to |(Sub(inst_expr)(1))..(Sub(inst_expr)(n))|.
In other words, in the expression:

%%[[wrap=code
let  id = def_expr
 in  ... (Sub(id)(1)) ... (Sub(id)(n)) ...
%%]

The definition for |id| is inlined:

%%[[wrap=code
... (Sub(inst_expr(1)) ... (Sub(inst_expr(n)) ...
%%]

We now compare the constraint sets resulting from constraint gathering on |def_expr| and each |(Sub(inst_expr)(i))|. The constraint
sets of such an |(Sub(inst_expr)(i))| is a copy of |S|, except that the uniqueness variables are different. The reason for this difference
is due to the fresh annotations of the types occurring in |(Sub(inst_expr)(1))..(Sub(inst_expr)(n))|, compared to the types in
|def_expr| (Section~\ref{fill this in - previous section}).

There is one other difference in the constraint set of the whole expression. If |def_expr| uses some identifier |x| from an outer
binding group, then the |x| occurs |n| times more often in the inlined version. The corresponding |\+/| constraint will be |n|
times bigger. We solve this difference with a trick: when instantiating the constraint set of |def_expr|, the uniqueness variables
occurring on the use-site-types of identifiers are not given fresh variables, unless they are an argument or result of a function.
This gives us |n| of the smaller |\+/| constraints, slightly different, but giving the same results as the |n| times as big |\+/|
constraint. These uniqueness variables on the use-site-types of identifiers, we call the mono-annotations, and all mono-variables
occurring in a binding group (including nested binding groups), are collected for each binding-group and passed to the
inferencer. These details are omitted in the type rules.

The inferencer performs the constraint duplication. In the gathered constraint set, we insert a placeholder to represent the
place where the constraints will be inserted, by means of the following constraint:

%%[[wrap=code
Inst <binding_group_id> [(<definition_site_uniqueness_variable>, <use_site_uniqueness_variable>)]
%%]

This constraint is inserted at the use-site of an identifier. It specifies that we take the constraint set gathered for the
identifier and insert it at the use-site. We already mentioned that the constraint set is the same, but the annotations
differ. The list of pairs in the constraint specify which uniqueness variables in the constraint set of the definition-site are
mapped to which uniqueness variables of the use-site. The other uniqueness variables occurring in the definition-site constraint set
are replaced by fresh variables in the inferencer (Section~\ref{fill this in}). This gives us the following rule for identifiers:

...type rule for Var...

There is a slight complication here. Identifiers can refer to bindings, but to function parameters as well. How to deal with
those? In principle, uses of function parameters do not require a constraint set, since the connections between the uniqueness
variables occurring on the type of function arguments are specified by the constraint set of the caller of the function. So,
for function parameters, the constraint set can be kept empty. Technically speaking, an |Inst| constraint can be generated for
a function parameter, but the constraints it represents will only not tell anything new about the uniqueness variables on the
type of a function parameter. So, in our description, we do not make this difference for conciseness, but make the difference
in the implementation for performance reasons.

For the Let, we have the following type rule to deal with a Let:

... type rule for Let...

The term \emph{binding group} refers to the bindings of the Let. We assume that each Let is numbered, and this number is
identifies the binding group. In EH, each Let only contains a single binding group, but technically speaking, the Let
can be partitioned into multiple binding groups (such as in Haskell), without invalidating this text.

Similarly to the discussion of parameters of a function, we need to sum up the individual uses of an identifier in
subexpression, resulting in the generation of a |\+/| constraint for each binding. The second task of the Let is
to gather the constraint for the bindings. The input to the type inferencer is a constraint set for each binding
group.


\section{The inferencer revisited}

The inferencer processes the constraint sets of binding groups is the order such that if $A$ is a constraint set
containing an |Inst| constraint referencing binding group $B$, then $B$ is processed before $A$. In other
words: the binding groups are processed in the order specified by a topological sort on the dependency graph
between constraints. Since the binding groups are non-recursive, it is guaranteed that when the inferencer
processes a binding group, that all the binding groups it depends on are already processed.

The processing of a binding group consists essentially of getting rid of the |Inst| constraints, and
replace them by normal constraints. Then we end up with a normal constraint set again, and can continue with
the uniqueness inference as specified in the first halve of this chapter. However, replacing the |Inst|
constraint by the constraint set of the corresponding binding group, tends to let the number of constraints
explode. We take a different approach, by converting each constraint set to an equivalent (Section~\ref{fill this in - proof})
graph representation: the \emph{constraint graph}. By simplifying the graph, it becomes small enough for practical
use (Section~\ref{fill this in}.

The graph representation is rather simple: the uniqueness variables are nodes. The constraints are directed edges
between the nodes. The constraint |a =>= b| is translated to a directed edge from node |a| to node |b|. The constraint
|(Sub(a)(1)) \+/ ... \+/ (Sub(a)(n)) <= b| is translated to a hyper edge from nodes |(Sub(a)(1)), ..., (Sub(a)(n))| to
node |b|. The inference of the substitution is analogous to the inference on the constraint set, except that now
the edges of the graph are traversed instead of the constraints.

For example of a uniqueness graph, take the following constraint set:

%%[[wrap=code
constraint set
%%]

Converting this constraint set into a constraint graph gives the following picture:

%%[[wrap=code
picture
%%]

Performing uniqueness inference gives the following substitution:

%%[[wrap=code
picture
%%]

The inferencer now applies the following steps:

\begin{itemize}
\item Convert the constraints, excluding |Inst| constraints, of each binding group to a constraint graph.
\item For binding group |p|, with an |Inst q l| constraint, instantiate binding group |q| into |p|, with
definition-site uniqueness variables replaced by use-site uniqueness variables, as specified by |l|. What
basically happens is that for each node in |q|, a fresh node is inserted in |p|, except when it occurs
in |l|. Then each edge in |q| is inserted in |p|, taking the corresponding node in |p|.
\item Perform a fixpoint uniqueness inference on the constraint graph of the outermost binding group.
\end{itemize}

The last step results into a uniqueness substitution for each uniqueness variable occurring in types
of the outermost binding group of the program. It does not directly give a substitution for another
binding group, since that is potentially different for each use of an identifier occurring in such a
binding group. We can construct such a definition-site substitution from a use-site substitution. Suppose
we know the substitution $R$ for binding group |k|, and |k| contains the use-site of an identifier |x|
of binding group |j|. We can construct a substitution $S$ from $R$ and the |Inst|
constraint generated for |x|. We take $S$ such that it maps to |0| for each uniqueness variable of |j|,
except for the uniqueness variables mentioned by the |Inst| constraint. These we set to the value of
their use-site counterpart in $R$. By feeding $S$ as initial substitution into the inferencer,
to perform the fixpoint computation on the constraint graph of |j|, we get a final
substitution for each uniqueness variable of |j|. This allows us to get a use-site dependent typing for
a binding-group, which can be exploited by a code generator that can perform code specialization.


\section{Graph rewriting}

The performance of an implementation of the uniqueness type system is directly connected to the size of the
uniqueness graphs. Without special treatment, these graphs explode in size. For example, consider
Picture~\ref{create a picture}, which is a constraint graph\footnote{In a slightly different representation as used by our
prototype implementation.} for the program:

%%[[wrap=code
let  inc1 = \x -> x + 1
in   let  inc4 = \x -> inc1 (inc1 (inc1 (inc1 x))
     in   let  inc16 = \x -> inc4 (inc4 (inc4 (inc4 x))
          in   inc16 4
%%]

By applying several reduction strategies, we end up with the graph in Figure~\ref{create a picture}, which is
considerably smaller. The reason is that only mono-variables, and uniqueness variables that occur on the types of identifiers of
the bindings, matter. These are essentially all use-site and definition-site uniqueness variables. The other uniqueness variables
are intermediates, for example, taken freshly when instantiating a constraint graph. A constraint graph can be converted into an
equivalent constraint graph with all intermediate uniqueness variables eliminated, resulting only in constraints (or edges in the
constraint graph) between the uniqueness variables on the types of an identifier or the mono variables.

The theoretical maximum number of constraints between these \emph{sacred} uniqueness variables is only dependent on the
number of uniqueness variables, which in turn only depends on the size of the types in question, and the number of uses
of an identifier. Without elimination, the number of constraints depend on the amount of duplication, which can easily
cause the number of constraints to explode, as the example shows. With intermediate uniqueness variables elimination, and $n$
sacred uniqueness variables, the theoretical maximum number of normal edges is $O(n^2)$, but the theoretical number
of hyper edges is still high: $O(n!)$. In practice, these numbers are not reached. A uniqueness variable represents
memory, and memory in a program is largely independent. The constraint graphs are sparse, with the number of
constraints proportional to the number of uniqueness variables.

There are two moments to rewrite a constraint graph. Just before instantiation of a constraint graph, and after
all instantiations are done. In the first case, the graph is specialized to only those uniqueness variables that are
used in the new binding group, to prevent ending up with graphs that explode in size due to the structure of
an expression. For second case, the graph consists of the nodes and edges coming from the constraint set
of the binding group, and all nodes and edges coming from (rewritten) graphs of other binding groups. This
graph can have some redundancy, and by applying a set of rewrite rules, we reduce this graph. A smaller
graph of the second case, makes the procedure to obtain graphs of the first case, faster. We discuss both
reduction moments in more detail.

\subsection{Reducing instantiation graphs}

We reduce the graph of a binding group just before instantiating it. The uniqueness variables that have a
counterpart in the destination graph, are not reduced, they are \emph{sacred}. The other uniqueness
variables are intermediates and are eliminated.

Suppose $G$ represents the original graph, and suppose that $G$ does not contain any hyper edges. Further,
suppose that $H$ is the reduced graph. Only simple paths in $G$ between sacred
uniqueness variables matter for the reduction to $H$. Each simple path in $G$, not containing sacred
uniqueness variables as intermediate nodes, between
sacred uniqueness variables can be replaced by a single edge between the uniqueness variables in $H$, since
|p =>= ... (Sub(s)(i)) ... =>= q| equals |p =>= q|, as long as each uniqueness variable |(Sub(s)(i))|
is not a sacred uniqueness variable. A path in $G$ between sacred uniqueness variables with a sacred
uniqueness variable in between, does not matter, since |p =>= ... =>= r =>= ... =>= q| equals
|p =>= r, r =>= q|, which are already present in $H$ by the above procedure. The following example
illustrates this procedure:

%%[[wrap=code
... example ...
%%]

But, a uniqueness graph can contain hyper edges. Suppose that $G$ has hyper edges, and that we construct
a graph $F$ from $G$, such that $F$ only consists of all simple paths from $G$, on which there is a hyper edge, and no sacred uniqueness
variables as intermediate nodes, and only sacred uniqueness variables at both ends. A hyper edge in $F$
can have less branches than in $G$, if some branch is not on a path to a sacred uniqueness variable.

%%[[wrap=code
... example of F ...
%%]

Now, if we union the graphs $F$ and $H$, we have a graph on which uniqueness inference results into the
same solution for the sacred uniqueness variables, as uniqueness inference gives for $G$.

Graph $F$ can be reduced further. We would like to end up with only the hyper edges between sacred uniqueness
variables. However, this does not work, because there can be an intermediate uniqueness variable between
a sacred uniqueness variable and a hyper edge connected by a coercion, which cannot be eliminated. We can
reduce from $F$ each intermediate node not connected by a hyper edge. Due to this construction, each
normal edge contains exactly one sacred uniqueness variable, and the other uniqueness variable is connected
to a hyper edge. Call this graph $F'$. The number of nodes in $F'$ is therefore proportional to the number
of hyper edges between the sacred uniqueness variables.

The graph we use for instantiation is the union between $H$ and $F'$. This graph is considered to be in
\emph{normal form}. (todo: probably some more explanation about the normal form is required for later chapters)

If we look at the constraint sets of
which both graphs are a representation, then the constraint set of $H$ consists of the |=>=| constraints
between sacred uniqueness variables. $F'$ consists of the |\+/| constraints between sacred uniqueness
variables, with occasional a coercion. For example, suppose that |a|, |b|, and |c| are
sacred uniqueness variables. Then the constraint set of which $F'$ is a representation, can contain
constraints like |a \+/ b <= c|, but also |a =>= a', b =>= b', c' =>= c, a' \+/ b' <= c'|.

The graph can be reduced a bit further by eliminating subsumed hyper edges, or in other words,
subsumed |\+/| constraints. For example, suppose that we have |a \+/ b \+/ c <= d| and
|a \+/ b <= d|, then |a \+/ b <= d| is subsumed by |a \+/ b \+/ c <= d|, and can be erased. But, the
procedure of to obtain $F'$ already combined some hyper edges if they existed on the same path, such
that most hyper edges range over different sets of sacred uniqueness variables, which means that
subsumed hyper edges do not exist often, and the elimination does not contribute enough to the size
of the graph to be worthwhile.

In the next subsection, we discuss a reduction procedure that makes the graph of the binding group smaller,
such the procedure of this section works faster. The reduction procedure of this subsection has to be
performed for each instantiation, so if we can save time on this procedure, it helps to improve the
speed of the compiler. In fact, after the procedure of the next subsection has reduced the graph,
we can apply the procedure of this subsection once to this graph, taking all the uniqueness
variables on all types of the bindings of the binding group as sacred annotations. Then all the
path-finding work has to be done once (reducing the reduction work for instantiation to taking
a subgraph of the reduced graph, containing only those components that have annotations occurring
on the type of the binding that is instantiated), instead of each time again for an instantiation.
This saves time for binding groups with high dependencies between the identifiers, or when the
identifiers are used a lot.

\subsection{Reducing binding group graphs}

The constraint graph of a binding group is constructed by converting the constraints to their
graph representation, followed by the insertion of nodes and edges from instantiated graphs for
each |Inst| constraint. The resulting graph has several opportunities for reduction, such as
reducing a sequence of edges to a single edge, but most of these opportunities arise only in
subsequent chapters. The main motivation for reduction of this graph is to lower the size of
the graph to improve the performance of instantiations and fixpoint computations on the graph.

Again, some uniqueness variables may not be eliminated. The sacred uniqueness variables are those
that occur on the types of bindings in the binding group, and the mono variables. These are the
uniqueness variables that can have counter part in the destination graph during instantiation. The
other variables can be eliminated, since their values can be reconstructed once we have a substitution for the
sacred uniqueness variables, by filling in the values in the original graph, and performing the
fixpoint computation again, as we discussed earlier in this chapter.

A simple elimination strategy is splitting the constraint graph into connected components,
and dropping those components without a sacred uniqueness variable. These components cannot influence
the substitution on sacred uniqueness variables, since there is no path between them. This strategy
is only effective when there are multiple, independent (mutual exclusive), bindings in a binding group.
A possible use for this strategy arises in the context of a module system, where bindings from another
module can be considered to be in the same binding group.

Another elimination strategy is cycle reduction. But, cycles in the constraint graph cannot occur
in this chapter, since each binding is non-recursive. In Chapter~\ref{fill this in}, recursive Lets
are discussed, which can cause cycles in the constraint graph. A cycle in the constraint graph can be
reduced to all sacred nodes in the cycle, or to an arbitrary node if there are no sacred nodes in the
cycle. The edges of a singleton cycle, \emph{self edges}, can be removed entirely.

There is an effective rewrite strategy that can be applied in this chapter. Each non-sacred node |u| that
is not connected by hyper edges, can be eliminated. For each pair of nodes |a| and |b|, such that there
exists an edge |a| to |u|, and an edge from |u| to |b|, create an edge from |a| to |b|. Then remove
|u| and the corresponding edges from the graph. This strategy replaces intermediate nodes from the graph,
or in terms of constraint sets, removes redundant uniqueness nodes from the graph.

A final rewrite strategy that we discuss makes use of uniqueness information. By applying the first-stage
fixpoint computation on the constraint graph, we can already get lower bound reference counts on
uniqueness variables. Most of these values will still be |0|, since these values depend largely on the
usage of the binding group. But constraints from annotations of the user (Section~\ref{fill this in}), or
ways to deal with a module system (Section~\ref{fill this in}), can give some information that
allow |*| to be inferred as lower bound reference count. In that case, constraints between nodes with
such a uniqueness value can be erased. The effectiveness of this strategy remains a question.

The rewrites strategies discussed in this subsection, all have the property that they can be implemented in an efficient way. This
is important, since the performance of the procedure in Subsection~\ref{fill this in} depends largely
on the size of the input graph.

Note that we did not discuss reduction strategies for hyper edges. There are two reasons, the first
reason is that they do not form much of a problem in practice, due to the reduction work of
Subsection~\ref{fill this in}. The other reason is related to the first, in the sense that due to
the low priority of this subject, it remains still a research problem to properly reduce hyper
edges in a way that offers improvements in practice.

\subsection{Concluding remarks about graph reduction}

The concept of sacred uniqueness variables, or sacred nodes, is important. The number of sacred
uniqueness variables is a subset of all uniqueness variables involved, especially when
there are a lot of places where coercions occur (which creates intermediates). The reduction strategies
that we discussed, rewrite the graphs such that the size of the graph is proportional to the
number of sacred uniqueness variables. This helps to improve the performance of the compiler
on real-life programs. The number of sacred uniqueness variables is basically an indicator of
the required processing time of the uniqueness type system. To take this a step further, by
controlling the number of uniqueness variables, a compiler could trade accuracy conservatively
for compilation speed. For example, the use-site types can be replaced by definition-site types,
reducing the number of uniqueness annotations severely, but also reducing the quality of the
inference for these types. In Chapter~\ref{fill this in - data types} we introduce data types,
which leads to an explosion of sacred uniqueness variables, and we discuss several ways to allow control
over the number of uniqueness variables, and thus to control the size of the uniqueness graphs, and
the performance of the compiler.


\section{Conclusion}

The inferencing of uniqueness types is rather indirect. The typing problem is translated into a constraint problem, which
is turn is converted to a graph problem. This can be rather overkill at first thought, but it allows a certain separation
of concerns and abstraction, which is further exploited in subsequent chapters, but also in this chapter, we can use
this separation to allow non-recursive and monomorphic Let-bindings in the expression language.

The translation of the constraints to a graph representation allows another way of looking at the
typing problem. Paths in the constraint graph tell how uniqueness variables are related to each
other. Furthermore, we are only interested in the uniqueness values of a small subset of the nodes
of the constraint graph. By looking at the paths between these special, \emph{sacred}, nodes,
we eliminate the other, intermediate, nodes, such that the resulting constraint graphs are small,
which is important for the performance of the compiler.

In the next chapter, Chapter~\ref{fill this in}, the notion of a constraint graph will be exploited further,
to allow polymorphic and recursive Let bindings. The reduction work on graphs will become even more
important, as the recursion will cause a fixpoint construction of a constraint set of a binding group,
which would explode beyond imagination without the reduction work of this chapter.

%%]

