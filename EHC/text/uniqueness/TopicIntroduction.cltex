%%[main

\chapter{Introduction}

Jan is a student with only one passion in his life aside from programming: a daily shot of coffee in the morning. But, Jan is lazy
and does not want to bring his own coffee cup with him. Each time he buys a plastic cup, and throws it away afterwards, since he
does not need the cup anymore after a single use. One day Jan thinks about the cost of buying all these plastic cups and the cost for the government to collect
the waste. Jan suddenly gets a brilliant idea: recycling. If he changes the process such that he does not throw the cup away, but give it
back to machine, let the machine clean it now and then, and give it back to him the following day, then he does not need
to buy the plastic cups each time, and does not need to throw them away, then he can both be lazy and reduce
the cost of acquiring and disposing the cup.

Taking notice of this life lesson, consider pure, lazy functional programming languages. A program written in
such a language produces much more garbage than a program written in an imperative language. Referential
transparency is to blame. To guarantee referential transperancy, compilers generate code that treat values
as atomic. As a result, a destructive update is performed on a copy of the original value. The old values
are garbage collected when they are not used anymore. Suppose that a program is only interested in the
most up-to-date value. Directly after copying, the old value becomes garbage. But, if we directly recycle
the old value, then a copy does not need to be made. The result is less copying and less garbage. The
identification of which values can be safely recycled without violating referential transparency, is the
focus of this thesis.


\section{Uniqueness and Typing}

%format linearannot = "\bullet"
%format nonlinearannot = "\circ"

  Usage analysis is the topic of research that deals with the identification of values that can be recycled. Usage analysis determines
  from a program how often certain values are used. Such an analysis is formulated by means of a type system. In linear
  typing~\cite{fill this in}, the types are partitioned in linear types and non-linear types. A linear type is exactly used once and
  non-linear types are used any number of times. Suppose linear integer is denoted as |(Sup(Int)(linearannot))| and a non-linear integer
  as |(Sup(Int)(nonlinearannot))|, then the following is a correctly lineary-typed program, assuming that the plus operator requires a
  linear left argument and a non-linear right argument:

%%[[wrap=code
  let  x :: (Sup(Int)(linearannot))
       x = 3

       y :: (Sup(Int)(nonlinearannot))
       y = 4
   in  (y (Sub(+)(l)) x (Sub(+)(l))) x
%%]

  Linear types open up a can of optimizations~\cite{fill this in1}\cite{fill this in1}\cite{fill this in1}\cite{fill this in1}\cite{fill this in1}\cite{fill this in1}\cite{fill this in1}\cite{fill this in1}.
  In case of the example, the implementation of addition can, instead of allocating new memory to store the result of the addition, reuse
  the memory of |y|. But these optimizations need a guarantee that a linear type is used exactly once. A linear type system is used to
  either verify that these types are correct, or to infer them.

  Production compilers such as GHC~\cite{fill this in}, typically perform some usage analysis on their core language~\cite{fill this in - update analysis}\cite{fil this in - linear types}\cite{fill this in - other thingie} in the back-end, but
  there are some compilers that perform the analysis in the front-end. For Clean~\cite{fill this in}, usage analysis is part of the language
  specification, called \emph{uniqueness typing}. Uniqueness typing and linear typing are virtually the same~\cite{fill this in}.

\section{Goals}

  The goal of this master thesis is to explore uniqueness typing in the context of Haskell using a constraint-based
  uniqueness type system. This allows us to distinguish the derivation of demands on types from the abstract syntax tree from the actual
  inferencing of uniqueness properties. We discuss the complications that certain language features cause in relative isolation.

  Our type system is an improvement of the type system of Clean, in the sense that we do not make
  the assumption that an identifier is at least used once, and we neither make the assumption that a component of a value is considered
  to be used more than its spine. The absence of these assumptions complicates the type system severely.
  
  The results of this master project is this master thesis discussing the uniqueness type system, and a prototype implementation in
  the EH compiler~\cite{fill this in}.

\section{Organisation of this thesis}

  This thesis is organized as follows. We start with our choice for EHC as starting point for our prototype. Then there are several
  chapters explainnig the type system. We introduce language features such as recursion and data types chapter by chapter. The chosen
  order allows us to discuss features in isolation, and to let earlier chapters pave the way for subsequent chapters. The following
  table lists the chapters and the main contents:

  \begin{tabular}{l||p{40em}}
  Chapter & Contents \\
  \hline
  Chapter~\ref{fill this in} & Constraint-based uniqueness type checking on a simply-typed lambda calculus. We show how types
                               are annotated, and how constraints are generated between the annotations on the types. In the end
                               we show how we can interpret the constraints to verify that the types are correctly annotated.\\
  Chapter~\ref{fill this in} & Polyvariant uniqueness-type inference on generated constraints, and the support for a monomorphic and
                               non-recursive Let. Constraints are translated to a graph representation, which we rewrite to prevent
                               constraint duplication.
                               \\
  Chapter~\ref{fill this in} & We extend the Let in this chapter to support recursion. Recursion has influence on the constraint
                               generation process, and we show how we can abstract from is by using a special instantiation
                               constraint. We show that there are several ways to deal with this instantiation constraint in
                               the uniqueness-type inferencer.\\
  Chapter~\ref{fill this in} & Support for full Haskell-like Lets, by supporting polymorhpism. We show that instantiation of
                               a polymorphic type to a type with more structure, gives complications and give several approaches
                               of how to deal with this problem.\\
  Chapter~\ref{fill this in} & In this chapter, we add data types. We do not need much new concepts in order to support
                               data types, but the complications on the type level make it an involving change. It also requires
                               a thorough understanding of Chapter~\ref{fill this in} and Chapter~\ref{fill this in}, and
                               requires some minor concepts of the other earlier chapters. We also discuss
                               a case-statement in this chapter, and show how to deal with parallel execution paths. \\
  Chapter~\ref{fill this in} & We show how we can use the type system on the type level for analyses on data types. As an
                               example, we show how we can use it to determine polarity (variance) for data types of Haskell.\\
  Chapter~\ref{fill this in} & How to deal with overloading is the last aspect of the type system that we discuss. We discuss
                               several approaches and their consequences.
  \end{tabular}

  The remaining chapters look at the type system from a more practical view. How to use the types to optimize code, how to
  explain to the programmer why and how a certain uniqueness property is derived, and we finish with some performance
  issues that need to be taken care of in order to implement the type system in a real production compiler.

%%]

