%%[main

\chapter{Introduction}
\label{chapt.Introduction}

Jan is a student with, aside from programming, only one passion in his life: a daily shot of coffee in the morning. But, Jan is lazy
and does not want to bring his own coffee cup with him. Each time he buys a plastic cup and throws it away afterwards, since he
does not need the cup anymore after a single use. One day Jan thinks about the cost of buying all these plastic cups and the cost for the government to collect
the waste. Jan suddenly gets a brilliant idea: recycling. If he changes the process such that he does not throw the cup away, but give it
back to the machine, let the machine clean it now and then, and give it back to him the following day, then he does not need
to buy the plastic cups each time, and does not need to throw them away. This gives him the best of both worlds. He can both be lazy and reduce
the cost of acquiring and disposing the cup.

Taking notice of this life lesson, consider lazy purely functional programming languages. A program written in
such a language produces much more garbage than a program written in an imperative language. Referential
transparency is to blame. With referential transparency, any expression may be replaced with its result
without changing the behavior of the program. Consider the expression:

%%[[wrap=code
let  a   = array (1, 10000) (zip [1..] (repeat 5))
     a'  = a \\ [(1, 10)]
in   (a, a')
%%]

Suppose that |a| is a huge array with the value five for each element. The array |a'| is obtained from |a| by
changing the first element to ten. Suppose that this is done by updating the memory where |a| points to, which
is typical for assignments in imperative languages.

Referential transparency tells us that we are allowed to replace an expression by its value. Due to the
side-effect, |a| and |a'| point to the same value. So, the behaviour of this program is that a tupple
is constructor where the first component points to an array where the first element has the value ten.

But, if we evaluate this expression by beta reduction, we get a different result. If we beta reduce the
above expression one step, we get the expression:

%%[[wrap=code
(  array (1, 10000) (zip [1..] (repeat 5))
,  array (1, 10000) (zip [1..] (repeat 5)) \\ [(1, 10)] )
%%]

Further beta reduction results in a tupple being constructed with the first component pointing to an
array with the first component haaving the value five. This outcome is different, so referential transparency
does not hold! The problem is that we changed the value where |a| pointed to, in order to obtain
the value of |a'|.

To guarantee referential transparency, compilers generate code that treat values as atomic. In other
words, code is generated in such a way that the value where |a| points to, is never changed. This
is done by making a copy of the original value before performing a destructive update. So, in our
example, a copy of |a| is made before updating it. This has a considerable impact on performance.

However, performing a destructive update without making a copy, does not always violate referential
transparency. Consider the following variation of the above expression:

%%[[wrap=code
let  a   =  array (1, 10000) (zip [1..] (repeat 5))
     a'  =  a \\ [(1, 10)]
 in  a'
%%]

Call the value where |a| points to |v|. |v| is only used once. After making a copy of |v| to perform the destructive updaten,
|v| becomes garbage, since there are no references to it anymore. If we directly recycle |v|, then a copy does not need to be
made, and |v| can be changed instead. Referential transparency holds in this case (verify this).

The lesson that we learn here is that value that are used at most once, can by recycled or updated without
violating referential transparency. This results into less copying and less garbage, with the result that
functional programs run faster and use less memory. The identification of values that are used at most once,
is the focus of this master thesis.


\section{Usage analysis, typing, and uniqueness}

%format linearannot = "\bullet"
%format nonlinearannot = "\circ"

  Usage analysis is the topic of research that deals with the identification of values that can be recycled. The interest for
  usage analysis originated from the inefficiency of updating large data structures in functional programming languages, such
  as arrays. A copy of the entire array is made just to update a single element. Because of this, arrays in functional
  programming languages have a bad impact on performance. A diversity of approaches to eliminate the inefficiency were
  investigated~\cite{wadler86new}. Among these approaches, usage analysis plays an important role, since values that are
  at most used once can be updated without making a copy, removing the inefficiency effectively.

  Usage analysis on values turns out to be difficult as the representation of a value can be infinite. Lazy infinite
  lists are commonly used in Haskell, like |fibs = 0 : 1 : zipWith (+) fibs (tail fibs)|, but how can uniqueness properties
  of such a value be described? With first-order logic, some invariants can be defined over infinite values, but the
  inference of such invariants from the definition of |fibs| is difficult, and the verification of invariants
  undecidable. But, the properties on values can be approximated by describing the properties on the types of these
  values. The fibonacci sequence may be infinite, but its type |List Int| is not. The type constructors in the type,
  in this case |List| and |Int|, can be individually given a usage property, for example |(Sup(List)(20)) (Sup(Int)(10))|,
  where the List is considered to be used up to twenty times, and each element of the list up to ten times, for some program
  that only evaluates a few fibonacci numbers. Note that this is an approximation of the original problem, since the |Int|
  ranges over all elements of the list, and each element gets the same property, although given our intuition of the
  definition of |fibs|, lower numbers are used much more than higher numbers, since the lower numbers are used to obtain
  a higher number. The approximation makes it easier to deal with usage properties, hence usage analyses are defined in
  terms of type systems.

  But, natural numbers as an annotation on a type are still too difficult to handle. If we know such a number, we also
  know that a program terminates, and the verification of termination is known to be undecidable for a language
  like Haskell. It is not always possible to know an exact number. Again, an approximation is needed.
  A usage property of zero is interesting for a compiler, since that means that a compiler does not need to
  generate code for it. Likewise, knowing that a value is used once is useful as we mentioned in the beginning of
  this chapter. But knowing that a value is used twice, thrice or maybe a zillion times is not interesting for us,
  since that just means that the optimizations are not possible on such a value. So, a typical approximation is
  to use |0|, respectively |1|, if we are certain that some value is not used, respectively only used once. If
  the number of uses is more than once, or cannot be determined, the symbol |*| is used to represent an
  arbitrary number of usages.

  This approximation is clearly visible in linear typing~\cite{wadler90linear} (which has roots in linear logic~\cite{girard95linear}).
  In linear typing, values of a non-linear type can be used an arbitrary number of times, but for a type to be linear, the
  corresponding values must be used exactly once. If we demand that all types are linear, we get a pure linear type
  system (see Figure~\ref{RulerUniquenessExamples.L.expr.base}). An elegant aspect of this
  type system is that it is just a conventional type system with restrictions on the environment that guarantee that
  each identifier is used exactly once. In practice, a linear type system mixes linear type and non-linear types, demanding
  the environment restrictions on linear types, and no restrictions for non-linear types (see the type systems given
  by Wadler~\cite{wadler90linear} for more information). In this thesis, we assume that linear types are annotated with a |1|
  (i.e. |(Sup(Int)(1))|), and non-linear types with a |*| (i.e. |(Sup(Int)(*))|).

  \rulerCmdUse{RulerUniquenessExamples.L.expr.base}

  Linear types open up a can of optimizations~\cite{DBLP:conf/icfp/JonesPS96,DBLP:journals/entcs/Santos98,DBLP:conf/ifip/GillJ94,DBLP:conf/esop/Jones96a}.
  Let us consider a few of them. Linear typing solves the problem of array updates, since an array with a linear type can be updated without making a copy.
  But there are also optimalizations for smaller values. A function is strict in its linear arguments, so any linear argument can be evaluated before passing
  it to a function. The place where a linear value is used, is known, so code can be inserted to allocate the value on a heap that is not
  garbage collected, and code inserted to manually deallocate the value after its use. Expressions with a linear type - functions that
  are used exactly once for example - can be safely inlined without causing the inliner to loop. These are all optimizations that improve
  the memory utilization and runtime of functional programs.

  Clean~\cite{plasmeijer01clean-rep-2} uses the results of usage analysis for another purpose besides optimization, namely for their
  IO model~\cite{achten95ins}. The usage analysis of Clean is called uniqueness typing, although it is virtually the same as linear
  typing~\cite{barendsen96uniqueness, DBLP:journals/tcs/Harrington06}. A type is called unique if it is used
  at most once (i.e. affine instead of linear). Uniqueness typing is important for Clean since its IO model depends on it.
  Haskell uses monads for IO~\cite{DBLP:conf/icfp/JonesPS96}. A monad guarantees that interaction with the outside-world is
  single threaded, or in other words, that there is a guaranteed evaluation order on computations that interact with the
  outside world, which is made explicit with the |do| notation. Clean does not use monads, but use their uniqueness typing to
  enforce that the interaction with the outside world is single threaded. The world is represented by value of the type
  |(Sup(World)(1))|. The initial function of a program gets a world but has to return it as well\footnote{In this thesis, we write all code in Haskell-like notation}:
  |main :: (Sup(World)(1)) -> (Sup(World)(1))|. Any function with side-effect, such as
  |readline :: (Sup(World)(1)) -> (String, (Sup(World)(1)))|, requires the current world as parameter and returns a new world.
  Suppose that |w| is an identifier representing a unique world. Passing |w| to a function means that |w| cannot be used
  elsewhere. Since the main function demands a unique world as result, this means that if we pass |w| to a function, the
  function has to return a unique world, otherwise there is no way to get a unique world. This efectively means that a
  world is threaded through all function calls, which ensures an evaluation order between functions with side-effects.

  What distinguishes Clean is that the uniqueness typing is part of the language and performed in the front-end of the compiler. The programmer can
  interact with the uniqueness typesystem by writing uniqueness type signatures, or inspecting the signatures returned
  by the type inferencer. Production compilers such as GHC~\cite{DBLP:conf/fp/HallHPJW92}, typically perform some form usage
  analysis (linear type inference for example) on their core language in the back-end of the compiler, but that is not
  visible to the programmer and does not offer guarantees that certain usage properties were derived by the compiler.

\section{Goals}

  The goal of this master thesis is to explore uniqueness typing in the context of Haskell using a constraint-based
  uniqueness type system. Our implementation has features similar to the uniqueness typing of Clean, but there are
  some differences and we present some improvements.

  Clean uses a mechanism called ``marking'' that analyses term graphs to find out how often an identifier
  occurs on execution paths. These marks are converted to uniqueness types and these uniqueness types are propagated
  according to the type system. In our approach, we do not have a separate procedure for marking, but have a constraint
  that provides similar functionality. Our version of marking is defined in a more conventional notation of a typed
  abstract syntax tree of the program (Chapter~\ref{chapt.Parallel}).

  An improvement is that we consider the components of a value independent of the spine of the value. In Clean, if
  a component of a value is unique, then the spine of the value must neccessairly be unique as well. This is an
  unfortunate restriction in the presence of partial application, as the result of passing a unique parameter must
  be unique as well. The order in which parameters are passed to a function matter in terms of uniqueness, since
  unique parameters are best passed as late as possible, because from that moment on, the function may not be shared
  anymore. Our approach does not have this restriction, although we pay a price for it in terms of complexity (Chapter~\ref{chapt.Polymorphic}).

  The results of this master project are this master thesis that explains how uniqueness typing can be integrated
  in a Haskell-like language, using conventional type theory, and a prototype implementation in the EH compiler~\cite{dijkstra04ehc-web}.

\section{Intended audience}

  The intended audience are second-year master students in the discipline of software technology with type
  systems as specialization. We do not give proofs of our results (we are not so naive to think that a
  master student will actually read those), but rather clarify our results with examples and argumentation.

\section{Organization of this thesis}

  This thesis is organized as follows. We start with our choice for EHC as starting point for our prototype. Then there are several
  chapters explaining the type system. We introduce language features such as recursion and data types chapter by chapter. The chosen
  order allows us to discuss features in isolation, and to let earlier chapters pave the way for subsequent chapters. The following
  table lists the chapters and the main contents:

  \begin{tabular}{l||p{40em}}
  Chapter & Contents \\
  \hline
  Chapter~\ref{chapt.NoBindings} & Constraint-based uniqueness type checking on a simply typed lambda calculus. We show how types
                               are annotated, and how constraints are generated between the annotations on the types. In the end
                               we show how we can interpret the constraints to verify that the types are correctly annotated.\\
  Chapter~\ref{chapt.Polyvariant} & Polyvariant uniqueness-type inference on generated constraints, and the support for a monomorphic and
                               non-recursive |let|. Constraints are translated to a graph representation, which we rewrite to prevent
                               constraint duplication.
                               \\
  Chapter~\ref{chapt.Recursion} & We extend the |let| in this chapter to support recursion. Recursion has influence on the constraint
                               generation process, and we show how we can abstract from it by using a special instantiation
                               constraint. We show that there are several ways to deal with this instantiation constraint in
                               the uniqueness-type inferencer.\\
  Chapter~\ref{chapt.Polymorphic} & Support for a full Haskell-like |let|, by supporting polymorphism. We show that instantiation of
                               a polymorphic type to a type with more structure, gives complications and give several approaches
                               to deal with this problem.\\
  Chapter~\ref{chapt.Parallel} & This chapter is yet another step towards the support for data types. In this chapter, we discuss how
                                 to deal with an |if-then-else| expression, which forces us to deal with parallel execution paths. This
                                 allows us to discuss this aspect of |case| expressions (Chapter~\ref{chapt.DataTypes}) in isolation.
                                 This chapter resembles somewhat the marking procedure of Clean~\cite{barendsen93conventional}.\\
  Chapter~\ref{chapt.DataTypes} & In this chapter, we add data types to the language. We do not need much new concepts in order to support
                               data types, but the complications on the type level make it an involving change. A thorough understanding of Chapter~\ref{chapt.NoBindings} and Chapter~\ref{chapt.Polyvariant}, and
                               some concepts of the other earlier chapters, is required.\\
  Chapter~\ref{chapt.Polarity} & We show how we can use the type system on the type level for analyses on data types. As an
                               example, we show how we can use it to determine polarity (variance) for data types of Haskell.\\
  Chapter~\ref{chapt.Overloading} & How to deal with overloading is the last aspect of the type system that we discuss. We discuss
                               several approaches and their consequences.\\
  Chapter~\ref{chap.InspectingResults} & There comes a time when the programmer is not satisfied with the results of the
                                         uniqueness typing, or wants to make sure that some result is enforced. Our type
                                         system is located in the front-end of the compiler, allowing interaction between
                                         the type system and the programmer. We discuss some mechanisms that the
                                         programmer can use to influence the type system, or to inspect the derived
                                         uniqueness types of a program.
  \end{tabular}

  Of these chapters, Chapter~\ref{chapt.NoBindings} and Chapter~\ref{chapt.Polyvariant} are required to understand the
  other chapters, and some understanding of these chapters is required to understand the others. The other chapters
  are incremental in the sense that they build on the results of previous chapters, but less heavily.

  Finally, as a sidenote, we want to disambiguate our uses of the word Haskell. With haskell, we mean Haskell 98
  as defined by Peyton Jones et al.~\cite{peytonjones03has98-rev-rep}. If required, we assume the
  presence of some commonly used extensions such as multiparameter type classes, functional dependencies, and
  higher-ranked types.

%%]
